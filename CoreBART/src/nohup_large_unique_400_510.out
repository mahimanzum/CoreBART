Source: source Target: target
2021-10-24 20:20:48 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.1, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../processed_data/large/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eval_bleu=True, eval_bleu_args='{"beam": 5}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, langs='java,python,en_XX', layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='json', log_interval=100, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=3000, max_sentences=4, max_sentences_valid=4, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=200000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=10, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='../plbart/checkpoint_11_100000.pt', save_dir='../models/plbart/unique/large', save_interval=1, save_interval_updates=0, seed=1234, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='source', stop_time_hours=0, target_lang='target', task='translation_without_lang_token', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', truncate_source=True, update_freq=[4], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir='../user_dir', valid_subset='valid', validate_interval=1, warmup_updates=500, weight_decay=0.0)
2021-10-24 20:20:48 | INFO | fairseq.tasks.translation | [source] dictionary: 50001 types
2021-10-24 20:20:48 | INFO | fairseq.tasks.translation | [target] dictionary: 50001 types
2021-10-24 20:20:48 | INFO | fairseq.data.data_utils | loaded 2449 examples from: ../processed_data/large/data-bin/valid.source-target.source
2021-10-24 20:20:48 | INFO | fairseq.data.data_utils | loaded 2449 examples from: ../processed_data/large/data-bin/valid.source-target.target
2021-10-24 20:20:48 | INFO | fairseq.tasks.translation | ../processed_data/large/data-bin valid source-target 2449 examples
2021-10-24 20:20:54 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-10-24 20:20:54 | INFO | fairseq_cli.train | model mbart_base, criterion LabelSmoothedCrossEntropyCriterion
2021-10-24 20:20:54 | INFO | fairseq_cli.train | num. model params: 139220736 (num. trained: 139220736)
2021-10-24 20:20:57 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-10-24 20:20:57 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-10-24 20:20:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-10-24 20:20:57 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 12.000 GB ; name = GRID P40-12Q                            
2021-10-24 20:20:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-10-24 20:20:57 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-10-24 20:20:57 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 4
2021-10-24 20:21:02 | INFO | fairseq.trainer | loaded checkpoint ../plbart/checkpoint_11_100000.pt (epoch 11 @ 0 updates)
2021-10-24 20:21:02 | INFO | fairseq.optim.adam | using FusedAdam
2021-10-24 20:21:02 | INFO | fairseq.trainer | loading train data for epoch 1
2021-10-24 20:21:02 | INFO | fairseq.data.data_utils | loaded 19590 examples from: ../processed_data/large/data-bin/train.source-target.source
2021-10-24 20:21:02 | INFO | fairseq.data.data_utils | loaded 19590 examples from: ../processed_data/large/data-bin/train.source-target.target
2021-10-24 20:21:02 | INFO | fairseq.tasks.translation | ../processed_data/large/data-bin train source-target 19590 examples
2021-10-24 20:21:02 | INFO | fairseq_cli.train | begin training epoch 1
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2021-10-24 20:21:35 | INFO | train_inner | {"epoch": 1, "update": 0.082, "loss": "6.535", "nll_loss": "4.489", "ppl": "22.45", "wps": "1055.2", "ups": "3.09", "wpb": "341.4", "bsz": "16", "num_updates": "100", "lr": "1e-05", "gnorm": "51.41", "train_wall": "33", "wall": "38"}
2021-10-24 20:22:09 | INFO | train_inner | {"epoch": 1, "update": 0.163, "loss": "4.02", "nll_loss": "2.034", "ppl": "4.1", "wps": "834.1", "ups": "2.93", "wpb": "284.3", "bsz": "16", "num_updates": "200", "lr": "2e-05", "gnorm": "11.076", "train_wall": "34", "wall": "72"}
2021-10-24 20:22:44 | INFO | train_inner | {"epoch": 1, "update": 0.245, "loss": "3.27", "nll_loss": "1.267", "ppl": "2.41", "wps": "950.7", "ups": "2.9", "wpb": "327.5", "bsz": "16", "num_updates": "300", "lr": "3e-05", "gnorm": "5.834", "train_wall": "34", "wall": "106"}
2021-10-24 20:23:16 | INFO | train_inner | {"epoch": 1, "update": 0.327, "loss": "3.277", "nll_loss": "1.353", "ppl": "2.55", "wps": "789.2", "ups": "3.04", "wpb": "259.9", "bsz": "16", "num_updates": "400", "lr": "4e-05", "gnorm": "4.423", "train_wall": "33", "wall": "139"}
2021-10-24 20:23:51 | INFO | train_inner | {"epoch": 1, "update": 0.408, "loss": "2.977", "nll_loss": "1.034", "ppl": "2.05", "wps": "1005.2", "ups": "2.91", "wpb": "345.6", "bsz": "16", "num_updates": "500", "lr": "5e-05", "gnorm": "3.813", "train_wall": "34", "wall": "174"}
2021-10-24 20:24:23 | INFO | train_inner | {"epoch": 1, "update": 0.49, "loss": "3.035", "nll_loss": "1.122", "ppl": "2.18", "wps": "902.8", "ups": "3.13", "wpb": "288.4", "bsz": "16", "num_updates": "600", "lr": "4.9995e-05", "gnorm": "3.737", "train_wall": "32", "wall": "206"}
2021-10-24 20:24:56 | INFO | train_inner | {"epoch": 1, "update": 0.571, "loss": "2.824", "nll_loss": "0.891", "ppl": "1.85", "wps": "1074.6", "ups": "2.97", "wpb": "361.5", "bsz": "16", "num_updates": "700", "lr": "4.999e-05", "gnorm": "3.275", "train_wall": "33", "wall": "239"}
2021-10-24 20:25:32 | INFO | train_inner | {"epoch": 1, "update": 0.653, "loss": "2.91", "nll_loss": "0.983", "ppl": "1.98", "wps": "944.8", "ups": "2.83", "wpb": "333.9", "bsz": "16", "num_updates": "800", "lr": "4.9985e-05", "gnorm": "3.473", "train_wall": "35", "wall": "275"}
2021-10-24 20:26:04 | INFO | train_inner | {"epoch": 1, "update": 0.735, "loss": "2.892", "nll_loss": "0.98", "ppl": "1.97", "wps": "1003.2", "ups": "3.12", "wpb": "322", "bsz": "16", "num_updates": "900", "lr": "4.998e-05", "gnorm": "3.463", "train_wall": "32", "wall": "307"}
2021-10-24 20:26:35 | INFO | train_inner | {"epoch": 1, "update": 0.816, "loss": "3.034", "nll_loss": "1.149", "ppl": "2.22", "wps": "823.9", "ups": "3.19", "wpb": "258.5", "bsz": "16", "num_updates": "1000", "lr": "4.9975e-05", "gnorm": "3.497", "train_wall": "31", "wall": "338"}
2021-10-24 20:27:08 | INFO | train_inner | {"epoch": 1, "update": 0.898, "loss": "2.891", "nll_loss": "0.989", "ppl": "1.98", "wps": "944.3", "ups": "3.06", "wpb": "308.7", "bsz": "16", "num_updates": "1100", "lr": "4.997e-05", "gnorm": "3.073", "train_wall": "32", "wall": "371"}
2021-10-24 20:27:42 | INFO | train_inner | {"epoch": 1, "update": 0.98, "loss": "2.86", "nll_loss": "0.958", "ppl": "1.94", "wps": "907.5", "ups": "2.91", "wpb": "311.7", "bsz": "16", "num_updates": "1200", "lr": "4.9965e-05", "gnorm": "3.296", "train_wall": "34", "wall": "405"}
2021-10-24 20:27:50 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 20:27:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 20:31:01 | INFO | valid | {"epoch": 1, "valid_loss": "2.923", "valid_nll_loss": "0.866", "valid_ppl": "1.82", "valid_bleu": "63.34", "valid_wps": "238.4", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "1225"}
2021-10-24 20:31:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 20:31:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_best.pt (epoch 1 @ 1225 updates, score 63.34) (writing took 8.82524796796497 seconds)
2021-10-24 20:31:10 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-10-24 20:31:10 | INFO | train | {"epoch": 1, "train_loss": "3.384", "train_nll_loss": "1.443", "train_ppl": "2.72", "train_wps": "625.7", "train_ups": "2.02", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "1225", "train_lr": "4.99637e-05", "train_gnorm": "8.265", "train_train_wall": "405", "train_wall": "613"}
2021-10-24 20:31:10 | INFO | fairseq_cli.train | begin training epoch 1
2021-10-24 20:31:37 | INFO | train_inner | {"epoch": 2, "update": 1.061, "loss": "2.834", "nll_loss": "0.927", "ppl": "1.9", "wps": "119.6", "ups": "0.43", "wpb": "280.2", "bsz": "15.9", "num_updates": "1300", "lr": "4.996e-05", "gnorm": "3.237", "train_wall": "34", "wall": "639"}
2021-10-24 20:32:11 | INFO | train_inner | {"epoch": 2, "update": 1.143, "loss": "2.755", "nll_loss": "0.838", "ppl": "1.79", "wps": "900.1", "ups": "2.87", "wpb": "313.3", "bsz": "16", "num_updates": "1400", "lr": "4.9955e-05", "gnorm": "3.165", "train_wall": "35", "wall": "674"}
2021-10-24 20:32:44 | INFO | train_inner | {"epoch": 2, "update": 1.224, "loss": "2.723", "nll_loss": "0.815", "ppl": "1.76", "wps": "949.8", "ups": "3.03", "wpb": "313.6", "bsz": "16", "num_updates": "1500", "lr": "4.995e-05", "gnorm": "3.044", "train_wall": "33", "wall": "707"}
2021-10-24 20:33:21 | INFO | train_inner | {"epoch": 2, "update": 1.306, "loss": "2.75", "nll_loss": "0.843", "ppl": "1.79", "wps": "904.6", "ups": "2.75", "wpb": "329.5", "bsz": "16", "num_updates": "1600", "lr": "4.9945e-05", "gnorm": "3.12", "train_wall": "36", "wall": "744"}
2021-10-24 20:33:58 | INFO | train_inner | {"epoch": 2, "update": 1.388, "loss": "2.681", "nll_loss": "0.766", "ppl": "1.7", "wps": "850", "ups": "2.72", "wpb": "312.2", "bsz": "16", "num_updates": "1700", "lr": "4.994e-05", "gnorm": "2.925", "train_wall": "36", "wall": "780"}
2021-10-24 20:34:34 | INFO | train_inner | {"epoch": 2, "update": 1.469, "loss": "2.673", "nll_loss": "0.766", "ppl": "1.7", "wps": "844.9", "ups": "2.74", "wpb": "308.1", "bsz": "16", "num_updates": "1800", "lr": "4.9935e-05", "gnorm": "2.833", "train_wall": "36", "wall": "817"}
2021-10-24 20:35:09 | INFO | train_inner | {"epoch": 2, "update": 1.551, "loss": "2.747", "nll_loss": "0.843", "ppl": "1.79", "wps": "908.1", "ups": "2.88", "wpb": "315.5", "bsz": "16", "num_updates": "1900", "lr": "4.993e-05", "gnorm": "3.183", "train_wall": "34", "wall": "851"}
2021-10-24 20:35:45 | INFO | train_inner | {"epoch": 2, "update": 1.633, "loss": "2.729", "nll_loss": "0.826", "ppl": "1.77", "wps": "815", "ups": "2.74", "wpb": "297.7", "bsz": "16", "num_updates": "2000", "lr": "4.9925e-05", "gnorm": "3.231", "train_wall": "36", "wall": "888"}
2021-10-24 20:36:19 | INFO | train_inner | {"epoch": 2, "update": 1.714, "loss": "2.737", "nll_loss": "0.84", "ppl": "1.79", "wps": "951", "ups": "2.94", "wpb": "323.7", "bsz": "16", "num_updates": "2100", "lr": "4.992e-05", "gnorm": "3.127", "train_wall": "34", "wall": "922"}
2021-10-24 20:36:53 | INFO | train_inner | {"epoch": 2, "update": 1.796, "loss": "2.733", "nll_loss": "0.833", "ppl": "1.78", "wps": "927", "ups": "2.93", "wpb": "316.8", "bsz": "16", "num_updates": "2200", "lr": "4.9915e-05", "gnorm": "3.125", "train_wall": "34", "wall": "956"}
2021-10-24 20:37:28 | INFO | train_inner | {"epoch": 2, "update": 1.878, "loss": "2.697", "nll_loss": "0.794", "ppl": "1.73", "wps": "871.6", "ups": "2.89", "wpb": "302.1", "bsz": "16", "num_updates": "2300", "lr": "4.991e-05", "gnorm": "3.156", "train_wall": "34", "wall": "991"}
2021-10-24 20:38:04 | INFO | train_inner | {"epoch": 2, "update": 1.959, "loss": "2.753", "nll_loss": "0.859", "ppl": "1.81", "wps": "863.7", "ups": "2.81", "wpb": "307.1", "bsz": "16", "num_updates": "2400", "lr": "4.9905e-05", "gnorm": "3.212", "train_wall": "35", "wall": "1026"}
2021-10-24 20:38:20 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 20:38:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 20:41:32 | INFO | valid | {"epoch": 2, "valid_loss": "2.861", "valid_nll_loss": "0.812", "valid_ppl": "1.76", "valid_bleu": "64.3", "valid_wps": "235.8", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "2450", "valid_best_bleu": "64.3"}
2021-10-24 20:41:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 20:41:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_best.pt (epoch 2 @ 2450 updates, score 64.3) (writing took 24.333600874990225 seconds)
2021-10-24 20:41:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-10-24 20:41:56 | INFO | train | {"epoch": 2, "train_loss": "2.731", "train_nll_loss": "0.826", "train_ppl": "1.77", "train_wps": "588.2", "train_ups": "1.9", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "2450", "train_lr": "4.99025e-05", "train_gnorm": "3.126", "train_train_wall": "427", "train_wall": "1259"}
2021-10-24 20:41:56 | INFO | fairseq_cli.train | begin training epoch 2
2021-10-24 20:42:14 | INFO | train_inner | {"epoch": 3, "update": 2.041, "loss": "2.618", "nll_loss": "0.705", "ppl": "1.63", "wps": "131.3", "ups": "0.4", "wpb": "329.1", "bsz": "15.9", "num_updates": "2500", "lr": "4.98999e-05", "gnorm": "3.529", "train_wall": "34", "wall": "1277"}
2021-10-24 20:42:49 | INFO | train_inner | {"epoch": 3, "update": 2.122, "loss": "2.587", "nll_loss": "0.665", "ppl": "1.59", "wps": "929.6", "ups": "2.91", "wpb": "319.5", "bsz": "16", "num_updates": "2600", "lr": "4.98949e-05", "gnorm": "3.082", "train_wall": "34", "wall": "1312"}
2021-10-24 20:43:24 | INFO | train_inner | {"epoch": 3, "update": 2.204, "loss": "2.588", "nll_loss": "0.671", "ppl": "1.59", "wps": "828.8", "ups": "2.8", "wpb": "296.3", "bsz": "16", "num_updates": "2700", "lr": "4.98899e-05", "gnorm": "3.003", "train_wall": "35", "wall": "1347"}
2021-10-24 20:44:01 | INFO | train_inner | {"epoch": 3, "update": 2.286, "loss": "2.582", "nll_loss": "0.667", "ppl": "1.59", "wps": "716.1", "ups": "2.76", "wpb": "259.9", "bsz": "16", "num_updates": "2800", "lr": "4.98849e-05", "gnorm": "2.981", "train_wall": "36", "wall": "1384"}
2021-10-24 20:44:37 | INFO | train_inner | {"epoch": 3, "update": 2.367, "loss": "2.554", "nll_loss": "0.633", "ppl": "1.55", "wps": "1006.1", "ups": "2.79", "wpb": "360", "bsz": "16", "num_updates": "2900", "lr": "4.98799e-05", "gnorm": "2.739", "train_wall": "36", "wall": "1419"}
2021-10-24 20:45:10 | INFO | train_inner | {"epoch": 3, "update": 2.449, "loss": "2.561", "nll_loss": "0.648", "ppl": "1.57", "wps": "998.8", "ups": "2.97", "wpb": "335.9", "bsz": "16", "num_updates": "3000", "lr": "4.98749e-05", "gnorm": "2.86", "train_wall": "33", "wall": "1453"}
2021-10-24 20:45:43 | INFO | train_inner | {"epoch": 3, "update": 2.531, "loss": "2.608", "nll_loss": "0.698", "ppl": "1.62", "wps": "872.8", "ups": "3.08", "wpb": "283.2", "bsz": "16", "num_updates": "3100", "lr": "4.98699e-05", "gnorm": "3.05", "train_wall": "32", "wall": "1485"}
2021-10-24 20:46:15 | INFO | train_inner | {"epoch": 3, "update": 2.612, "loss": "2.623", "nll_loss": "0.715", "ppl": "1.64", "wps": "860.7", "ups": "3.13", "wpb": "275.4", "bsz": "16", "num_updates": "3200", "lr": "4.98649e-05", "gnorm": "3.034", "train_wall": "32", "wall": "1517"}
2021-10-24 20:46:46 | INFO | train_inner | {"epoch": 3, "update": 2.694, "loss": "2.576", "nll_loss": "0.666", "ppl": "1.59", "wps": "966.1", "ups": "3.21", "wpb": "301", "bsz": "16", "num_updates": "3300", "lr": "4.98599e-05", "gnorm": "2.864", "train_wall": "31", "wall": "1549"}
2021-10-24 20:47:16 | INFO | train_inner | {"epoch": 3, "update": 2.776, "loss": "2.585", "nll_loss": "0.675", "ppl": "1.6", "wps": "1109.6", "ups": "3.35", "wpb": "331", "bsz": "16", "num_updates": "3400", "lr": "4.98549e-05", "gnorm": "2.914", "train_wall": "30", "wall": "1578"}
2021-10-24 20:47:44 | INFO | train_inner | {"epoch": 3, "update": 2.857, "loss": "2.619", "nll_loss": "0.717", "ppl": "1.64", "wps": "957.4", "ups": "3.51", "wpb": "272.5", "bsz": "16", "num_updates": "3500", "lr": "4.98499e-05", "gnorm": "3.099", "train_wall": "28", "wall": "1607"}
2021-10-24 20:48:17 | INFO | train_inner | {"epoch": 3, "update": 2.939, "loss": "2.51", "nll_loss": "0.594", "ppl": "1.51", "wps": "1092.1", "ups": "3.01", "wpb": "362.8", "bsz": "16", "num_updates": "3600", "lr": "4.98449e-05", "gnorm": "2.794", "train_wall": "33", "wall": "1640"}
2021-10-24 20:48:39 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 20:48:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 20:51:59 | INFO | valid | {"epoch": 3, "valid_loss": "2.842", "valid_nll_loss": "0.816", "valid_ppl": "1.76", "valid_bleu": "64.54", "valid_wps": "226.6", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "3675", "valid_best_bleu": "64.54"}
2021-10-24 20:51:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 20:52:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_best.pt (epoch 3 @ 3675 updates, score 64.54) (writing took 37.23872087197378 seconds)
2021-10-24 20:52:37 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-10-24 20:52:37 | INFO | train | {"epoch": 3, "train_loss": "2.578", "train_nll_loss": "0.664", "train_ppl": "1.58", "train_wps": "593.7", "train_ups": "1.91", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "3675", "train_lr": "4.98412e-05", "train_gnorm": "2.982", "train_train_wall": "400", "train_wall": "1899"}
2021-10-24 20:52:37 | INFO | fairseq_cli.train | begin training epoch 3
2021-10-24 20:52:46 | INFO | train_inner | {"epoch": 4, "update": 3.02, "loss": "2.532", "nll_loss": "0.618", "ppl": "1.53", "wps": "115.5", "ups": "0.37", "wpb": "310.6", "bsz": "15.9", "num_updates": "3700", "lr": "4.98399e-05", "gnorm": "2.99", "train_wall": "31", "wall": "1909"}
2021-10-24 20:53:24 | INFO | train_inner | {"epoch": 4, "update": 3.102, "loss": "2.428", "nll_loss": "0.495", "ppl": "1.41", "wps": "927", "ups": "2.66", "wpb": "348.5", "bsz": "16", "num_updates": "3800", "lr": "4.98349e-05", "gnorm": "2.708", "train_wall": "37", "wall": "1947"}
2021-10-24 20:53:59 | INFO | train_inner | {"epoch": 4, "update": 3.184, "loss": "2.472", "nll_loss": "0.545", "ppl": "1.46", "wps": "905.3", "ups": "2.82", "wpb": "320.5", "bsz": "16", "num_updates": "3900", "lr": "4.98299e-05", "gnorm": "2.788", "train_wall": "35", "wall": "1982"}
2021-10-24 20:54:35 | INFO | train_inner | {"epoch": 4, "update": 3.265, "loss": "2.464", "nll_loss": "0.538", "ppl": "1.45", "wps": "807", "ups": "2.83", "wpb": "285.5", "bsz": "16", "num_updates": "4000", "lr": "4.98249e-05", "gnorm": "3.077", "train_wall": "35", "wall": "2017"}
2021-10-24 20:55:12 | INFO | train_inner | {"epoch": 4, "update": 3.347, "loss": "2.437", "nll_loss": "0.507", "ppl": "1.42", "wps": "921.5", "ups": "2.65", "wpb": "348.3", "bsz": "16", "num_updates": "4100", "lr": "4.98199e-05", "gnorm": "2.762", "train_wall": "38", "wall": "2055"}
2021-10-24 20:55:46 | INFO | train_inner | {"epoch": 4, "update": 3.429, "loss": "2.484", "nll_loss": "0.561", "ppl": "1.47", "wps": "931.8", "ups": "3.01", "wpb": "309.2", "bsz": "16", "num_updates": "4200", "lr": "4.98149e-05", "gnorm": "3.026", "train_wall": "33", "wall": "2088"}
2021-10-24 20:56:19 | INFO | train_inner | {"epoch": 4, "update": 3.51, "loss": "2.438", "nll_loss": "0.512", "ppl": "1.43", "wps": "931.5", "ups": "2.98", "wpb": "313.1", "bsz": "16", "num_updates": "4300", "lr": "4.98099e-05", "gnorm": "2.951", "train_wall": "33", "wall": "2122"}
2021-10-24 20:56:53 | INFO | train_inner | {"epoch": 4, "update": 3.592, "loss": "2.474", "nll_loss": "0.553", "ppl": "1.47", "wps": "868", "ups": "2.93", "wpb": "296.1", "bsz": "16", "num_updates": "4400", "lr": "4.98049e-05", "gnorm": "3.089", "train_wall": "34", "wall": "2156"}
2021-10-24 20:57:25 | INFO | train_inner | {"epoch": 4, "update": 3.673, "loss": "2.482", "nll_loss": "0.561", "ppl": "1.48", "wps": "959", "ups": "3.18", "wpb": "301.4", "bsz": "16", "num_updates": "4500", "lr": "4.97999e-05", "gnorm": "3.222", "train_wall": "31", "wall": "2188"}
2021-10-24 20:57:59 | INFO | train_inner | {"epoch": 4, "update": 3.755, "loss": "2.486", "nll_loss": "0.568", "ppl": "1.48", "wps": "839.2", "ups": "2.93", "wpb": "286.1", "bsz": "16", "num_updates": "4600", "lr": "4.97949e-05", "gnorm": "3.028", "train_wall": "34", "wall": "2222"}
2021-10-24 20:58:33 | INFO | train_inner | {"epoch": 4, "update": 3.837, "loss": "2.482", "nll_loss": "0.567", "ppl": "1.48", "wps": "950.8", "ups": "2.95", "wpb": "322", "bsz": "16", "num_updates": "4700", "lr": "4.97899e-05", "gnorm": "2.793", "train_wall": "34", "wall": "2256"}
2021-10-24 20:59:08 | INFO | train_inner | {"epoch": 4, "update": 3.918, "loss": "2.533", "nll_loss": "0.62", "ppl": "1.54", "wps": "734.8", "ups": "2.82", "wpb": "260.5", "bsz": "16", "num_updates": "4800", "lr": "4.97849e-05", "gnorm": "3.275", "train_wall": "35", "wall": "2291"}
2021-10-24 20:59:46 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 20:59:46 | INFO | train_inner | {"epoch": 4, "update": 4.0, "loss": "2.489", "nll_loss": "0.57", "ppl": "1.48", "wps": "839.1", "ups": "2.67", "wpb": "314.5", "bsz": "15.9", "num_updates": "4900", "lr": "4.97799e-05", "gnorm": "2.965", "train_wall": "37", "wall": "2328"}
2021-10-24 20:59:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 21:03:09 | INFO | valid | {"epoch": 4, "valid_loss": "2.88", "valid_nll_loss": "0.839", "valid_ppl": "1.79", "valid_bleu": "65.37", "valid_wps": "223.2", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "4900", "valid_best_bleu": "65.37"}
2021-10-24 21:03:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 21:03:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_best.pt (epoch 4 @ 4900 updates, score 65.37) (writing took 22.83600430295337 seconds)
2021-10-24 21:03:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-10-24 21:03:31 | INFO | train | {"epoch": 4, "train_loss": "2.468", "train_nll_loss": "0.544", "train_ppl": "1.46", "train_wps": "580.5", "train_ups": "1.87", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "4900", "train_lr": "4.97799e-05", "train_gnorm": "2.961", "train_train_wall": "426", "train_wall": "2554"}
2021-10-24 21:03:31 | INFO | fairseq_cli.train | begin training epoch 4
2021-10-24 21:04:06 | INFO | train_inner | {"epoch": 5, "update": 4.082, "loss": "2.342", "nll_loss": "0.405", "ppl": "1.32", "wps": "123.7", "ups": "0.38", "wpb": "322.3", "bsz": "16", "num_updates": "5000", "lr": "4.97749e-05", "gnorm": "2.561", "train_wall": "34", "wall": "2589"}
2021-10-24 21:04:39 | INFO | train_inner | {"epoch": 5, "update": 4.163, "loss": "2.364", "nll_loss": "0.427", "ppl": "1.34", "wps": "859.7", "ups": "3.07", "wpb": "280.1", "bsz": "16", "num_updates": "5100", "lr": "4.97699e-05", "gnorm": "2.695", "train_wall": "32", "wall": "2622"}
2021-10-24 21:05:11 | INFO | train_inner | {"epoch": 5, "update": 4.245, "loss": "2.39", "nll_loss": "0.453", "ppl": "1.37", "wps": "824.9", "ups": "3.07", "wpb": "268.8", "bsz": "16", "num_updates": "5200", "lr": "4.97649e-05", "gnorm": "2.776", "train_wall": "32", "wall": "2654"}
2021-10-24 21:05:43 | INFO | train_inner | {"epoch": 5, "update": 4.327, "loss": "2.374", "nll_loss": "0.444", "ppl": "1.36", "wps": "928.1", "ups": "3.2", "wpb": "290.4", "bsz": "16", "num_updates": "5300", "lr": "4.97599e-05", "gnorm": "2.827", "train_wall": "31", "wall": "2685"}
2021-10-24 21:06:14 | INFO | train_inner | {"epoch": 5, "update": 4.408, "loss": "2.38", "nll_loss": "0.445", "ppl": "1.36", "wps": "1020.4", "ups": "3.21", "wpb": "318", "bsz": "16", "num_updates": "5400", "lr": "4.97549e-05", "gnorm": "2.935", "train_wall": "31", "wall": "2717"}
2021-10-24 21:06:51 | INFO | train_inner | {"epoch": 5, "update": 4.49, "loss": "2.396", "nll_loss": "0.465", "ppl": "1.38", "wps": "932.2", "ups": "2.7", "wpb": "344.8", "bsz": "16", "num_updates": "5500", "lr": "4.97499e-05", "gnorm": "2.907", "train_wall": "37", "wall": "2754"}
2021-10-24 21:07:24 | INFO | train_inner | {"epoch": 5, "update": 4.571, "loss": "2.374", "nll_loss": "0.447", "ppl": "1.36", "wps": "872.9", "ups": "3", "wpb": "290.5", "bsz": "16", "num_updates": "5600", "lr": "4.97449e-05", "gnorm": "2.8", "train_wall": "33", "wall": "2787"}
2021-10-24 21:08:02 | INFO | train_inner | {"epoch": 5, "update": 4.653, "loss": "2.415", "nll_loss": "0.484", "ppl": "1.4", "wps": "896.2", "ups": "2.67", "wpb": "335.5", "bsz": "16", "num_updates": "5700", "lr": "4.97399e-05", "gnorm": "3.058", "train_wall": "37", "wall": "2824"}
2021-10-24 21:08:38 | INFO | train_inner | {"epoch": 5, "update": 4.735, "loss": "2.404", "nll_loss": "0.476", "ppl": "1.39", "wps": "812.2", "ups": "2.72", "wpb": "298.5", "bsz": "16", "num_updates": "5800", "lr": "4.97349e-05", "gnorm": "2.814", "train_wall": "37", "wall": "2861"}
2021-10-24 21:09:10 | INFO | train_inner | {"epoch": 5, "update": 4.816, "loss": "2.396", "nll_loss": "0.468", "ppl": "1.38", "wps": "1000.7", "ups": "3.13", "wpb": "319.9", "bsz": "16", "num_updates": "5900", "lr": "4.97299e-05", "gnorm": "2.977", "train_wall": "32", "wall": "2893"}
2021-10-24 21:09:44 | INFO | train_inner | {"epoch": 5, "update": 4.898, "loss": "2.374", "nll_loss": "0.445", "ppl": "1.36", "wps": "1038.5", "ups": "3", "wpb": "346.1", "bsz": "16", "num_updates": "6000", "lr": "4.97249e-05", "gnorm": "2.776", "train_wall": "33", "wall": "2926"}
2021-10-24 21:10:12 | INFO | train_inner | {"epoch": 5, "update": 4.98, "loss": "2.439", "nll_loss": "0.518", "ppl": "1.43", "wps": "966.7", "ups": "3.46", "wpb": "279.3", "bsz": "16", "num_updates": "6100", "lr": "4.97199e-05", "gnorm": "3.291", "train_wall": "29", "wall": "2955"}
2021-10-24 21:10:20 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 21:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 21:13:33 | INFO | valid | {"epoch": 5, "valid_loss": "2.902", "valid_nll_loss": "0.879", "valid_ppl": "1.84", "valid_bleu": "65.74", "valid_wps": "235.1", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "6125", "valid_best_bleu": "65.74"}
2021-10-24 21:13:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 21:13:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_best.pt (epoch 5 @ 6125 updates, score 65.74) (writing took 26.04196135501843 seconds)
2021-10-24 21:13:59 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-10-24 21:13:59 | INFO | train | {"epoch": 5, "train_loss": "2.386", "train_nll_loss": "0.456", "train_ppl": "1.37", "train_wps": "606.1", "train_ups": "1.95", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "6125", "train_lr": "4.97186e-05", "train_gnorm": "2.869", "train_train_wall": "406", "train_wall": "3181"}
2021-10-24 21:13:59 | INFO | fairseq_cli.train | begin training epoch 5
2021-10-24 21:14:25 | INFO | train_inner | {"epoch": 6, "update": 5.061, "loss": "2.308", "nll_loss": "0.369", "ppl": "1.29", "wps": "150.5", "ups": "0.4", "wpb": "379.8", "bsz": "15.9", "num_updates": "6200", "lr": "4.97149e-05", "gnorm": "2.632", "train_wall": "33", "wall": "3208"}
2021-10-24 21:14:58 | INFO | train_inner | {"epoch": 6, "update": 5.143, "loss": "2.281", "nll_loss": "0.341", "ppl": "1.27", "wps": "982.4", "ups": "3.04", "wpb": "323.6", "bsz": "16", "num_updates": "6300", "lr": "4.97099e-05", "gnorm": "2.458", "train_wall": "33", "wall": "3241"}
2021-10-24 21:15:31 | INFO | train_inner | {"epoch": 6, "update": 5.224, "loss": "2.311", "nll_loss": "0.371", "ppl": "1.29", "wps": "884.5", "ups": "3", "wpb": "295.1", "bsz": "16", "num_updates": "6400", "lr": "4.97049e-05", "gnorm": "2.664", "train_wall": "33", "wall": "3274"}
2021-10-24 21:16:03 | INFO | train_inner | {"epoch": 6, "update": 5.306, "loss": "2.299", "nll_loss": "0.361", "ppl": "1.28", "wps": "1055.1", "ups": "3.17", "wpb": "333.1", "bsz": "16", "num_updates": "6500", "lr": "4.96998e-05", "gnorm": "2.56", "train_wall": "31", "wall": "3305"}
2021-10-24 21:16:38 | INFO | train_inner | {"epoch": 6, "update": 5.388, "loss": "2.322", "nll_loss": "0.386", "ppl": "1.31", "wps": "992.9", "ups": "2.83", "wpb": "351.1", "bsz": "16", "num_updates": "6600", "lr": "4.96948e-05", "gnorm": "2.893", "train_wall": "35", "wall": "3341"}
2021-10-24 21:17:13 | INFO | train_inner | {"epoch": 6, "update": 5.469, "loss": "2.319", "nll_loss": "0.385", "ppl": "1.31", "wps": "830", "ups": "2.85", "wpb": "291.5", "bsz": "16", "num_updates": "6700", "lr": "4.96898e-05", "gnorm": "2.812", "train_wall": "35", "wall": "3376"}
2021-10-24 21:17:44 | INFO | train_inner | {"epoch": 6, "update": 5.551, "loss": "2.321", "nll_loss": "0.387", "ppl": "1.31", "wps": "996.5", "ups": "3.22", "wpb": "309.7", "bsz": "16", "num_updates": "6800", "lr": "4.96848e-05", "gnorm": "2.76", "train_wall": "31", "wall": "3407"}
2021-10-24 21:18:18 | INFO | train_inner | {"epoch": 6, "update": 5.633, "loss": "2.329", "nll_loss": "0.392", "ppl": "1.31", "wps": "870.5", "ups": "2.99", "wpb": "290.9", "bsz": "16", "num_updates": "6900", "lr": "4.96798e-05", "gnorm": "2.739", "train_wall": "33", "wall": "3440"}
2021-10-24 21:18:51 | INFO | train_inner | {"epoch": 6, "update": 5.714, "loss": "2.321", "nll_loss": "0.389", "ppl": "1.31", "wps": "1065.3", "ups": "2.96", "wpb": "359.5", "bsz": "16", "num_updates": "7000", "lr": "4.96748e-05", "gnorm": "2.709", "train_wall": "34", "wall": "3474"}
2021-10-24 21:19:22 | INFO | train_inner | {"epoch": 6, "update": 5.796, "loss": "2.335", "nll_loss": "0.404", "ppl": "1.32", "wps": "908.1", "ups": "3.29", "wpb": "276.4", "bsz": "16", "num_updates": "7100", "lr": "4.96698e-05", "gnorm": "2.762", "train_wall": "30", "wall": "3505"}
2021-10-24 21:19:55 | INFO | train_inner | {"epoch": 6, "update": 5.878, "loss": "2.34", "nll_loss": "0.408", "ppl": "1.33", "wps": "908.7", "ups": "3.03", "wpb": "299.7", "bsz": "16", "num_updates": "7200", "lr": "4.96648e-05", "gnorm": "2.854", "train_wall": "33", "wall": "3538"}
2021-10-24 21:20:31 | INFO | train_inner | {"epoch": 6, "update": 5.959, "loss": "2.381", "nll_loss": "0.451", "ppl": "1.37", "wps": "772.8", "ups": "2.79", "wpb": "277", "bsz": "16", "num_updates": "7300", "lr": "4.96598e-05", "gnorm": "3.088", "train_wall": "36", "wall": "3573"}
2021-10-24 21:20:50 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 21:20:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 21:24:15 | INFO | valid | {"epoch": 6, "valid_loss": "2.932", "valid_nll_loss": "0.911", "valid_ppl": "1.88", "valid_bleu": "65", "valid_wps": "221.2", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "7350", "valid_best_bleu": "65.74"}
2021-10-24 21:24:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 21:24:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 6 @ 7350 updates, score 65.0) (writing took 14.07436637103092 seconds)
2021-10-24 21:24:29 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-10-24 21:24:29 | INFO | train | {"epoch": 6, "train_loss": "2.322", "train_nll_loss": "0.387", "train_ppl": "1.31", "train_wps": "603.5", "train_ups": "1.94", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "7350", "train_lr": "4.96573e-05", "train_gnorm": "2.76", "train_train_wall": "408", "train_wall": "3811"}
2021-10-24 21:24:29 | INFO | fairseq_cli.train | begin training epoch 6
2021-10-24 21:24:44 | INFO | train_inner | {"epoch": 7, "update": 6.041, "loss": "2.321", "nll_loss": "0.386", "ppl": "1.31", "wps": "107.2", "ups": "0.39", "wpb": "271.6", "bsz": "15.9", "num_updates": "7400", "lr": "4.96548e-05", "gnorm": "2.839", "train_wall": "34", "wall": "3827"}
2021-10-24 21:25:18 | INFO | train_inner | {"epoch": 7, "update": 6.122, "loss": "2.278", "nll_loss": "0.338", "ppl": "1.26", "wps": "952", "ups": "2.92", "wpb": "326", "bsz": "16", "num_updates": "7500", "lr": "4.96498e-05", "gnorm": "2.512", "train_wall": "34", "wall": "3861"}
2021-10-24 21:25:54 | INFO | train_inner | {"epoch": 7, "update": 6.204, "loss": "2.265", "nll_loss": "0.324", "ppl": "1.25", "wps": "810.2", "ups": "2.82", "wpb": "287.1", "bsz": "16", "num_updates": "7600", "lr": "4.96448e-05", "gnorm": "2.773", "train_wall": "35", "wall": "3897"}
2021-10-24 21:26:30 | INFO | train_inner | {"epoch": 7, "update": 6.286, "loss": "2.261", "nll_loss": "0.32", "ppl": "1.25", "wps": "891.9", "ups": "2.74", "wpb": "325.8", "bsz": "16", "num_updates": "7700", "lr": "4.96398e-05", "gnorm": "2.757", "train_wall": "36", "wall": "3933"}
2021-10-24 21:27:05 | INFO | train_inner | {"epoch": 7, "update": 6.367, "loss": "2.286", "nll_loss": "0.347", "ppl": "1.27", "wps": "942.1", "ups": "2.9", "wpb": "325.4", "bsz": "16", "num_updates": "7800", "lr": "4.96348e-05", "gnorm": "2.757", "train_wall": "34", "wall": "3968"}
2021-10-24 21:27:41 | INFO | train_inner | {"epoch": 7, "update": 6.449, "loss": "2.287", "nll_loss": "0.351", "ppl": "1.28", "wps": "733.8", "ups": "2.8", "wpb": "261.7", "bsz": "16", "num_updates": "7900", "lr": "4.96298e-05", "gnorm": "2.714", "train_wall": "35", "wall": "4003"}
2021-10-24 21:28:16 | INFO | train_inner | {"epoch": 7, "update": 6.531, "loss": "2.28", "nll_loss": "0.342", "ppl": "1.27", "wps": "838.6", "ups": "2.82", "wpb": "297.6", "bsz": "16", "num_updates": "8000", "lr": "4.96248e-05", "gnorm": "2.758", "train_wall": "35", "wall": "4039"}
2021-10-24 21:28:52 | INFO | train_inner | {"epoch": 7, "update": 6.612, "loss": "2.28", "nll_loss": "0.344", "ppl": "1.27", "wps": "805.4", "ups": "2.81", "wpb": "286.7", "bsz": "16", "num_updates": "8100", "lr": "4.96198e-05", "gnorm": "2.765", "train_wall": "35", "wall": "4074"}
2021-10-24 21:29:25 | INFO | train_inner | {"epoch": 7, "update": 6.694, "loss": "2.265", "nll_loss": "0.331", "ppl": "1.26", "wps": "981.7", "ups": "2.95", "wpb": "332.5", "bsz": "16", "num_updates": "8200", "lr": "4.96148e-05", "gnorm": "2.54", "train_wall": "34", "wall": "4108"}
2021-10-24 21:30:00 | INFO | train_inner | {"epoch": 7, "update": 6.776, "loss": "2.308", "nll_loss": "0.373", "ppl": "1.3", "wps": "910.4", "ups": "2.89", "wpb": "314.6", "bsz": "16", "num_updates": "8300", "lr": "4.96098e-05", "gnorm": "2.838", "train_wall": "34", "wall": "4143"}
2021-10-24 21:30:33 | INFO | train_inner | {"epoch": 7, "update": 6.857, "loss": "2.291", "nll_loss": "0.359", "ppl": "1.28", "wps": "940.1", "ups": "2.99", "wpb": "314.1", "bsz": "16", "num_updates": "8400", "lr": "4.96048e-05", "gnorm": "2.695", "train_wall": "33", "wall": "4176"}
2021-10-24 21:31:06 | INFO | train_inner | {"epoch": 7, "update": 6.939, "loss": "2.303", "nll_loss": "0.372", "ppl": "1.29", "wps": "1082", "ups": "3.09", "wpb": "350.7", "bsz": "16", "num_updates": "8500", "lr": "4.95998e-05", "gnorm": "2.964", "train_wall": "32", "wall": "4209"}
2021-10-24 21:31:28 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 21:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 21:34:38 | INFO | valid | {"epoch": 7, "valid_loss": "2.948", "valid_nll_loss": "0.948", "valid_ppl": "1.93", "valid_bleu": "64.73", "valid_wps": "237.8", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "8575", "valid_best_bleu": "65.74"}
2021-10-24 21:34:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 21:34:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 7 @ 8575 updates, score 64.73) (writing took 6.8958300380036235 seconds)
2021-10-24 21:34:45 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-10-24 21:34:45 | INFO | train | {"epoch": 7, "train_loss": "2.282", "train_nll_loss": "0.346", "train_ppl": "1.27", "train_wps": "616.5", "train_ups": "1.99", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "8575", "train_lr": "4.9596e-05", "train_gnorm": "2.731", "train_train_wall": "416", "train_wall": "4428"}
2021-10-24 21:34:45 | INFO | fairseq_cli.train | begin training epoch 7
2021-10-24 21:34:55 | INFO | train_inner | {"epoch": 8, "update": 7.02, "loss": "2.279", "nll_loss": "0.343", "ppl": "1.27", "wps": "131.6", "ups": "0.44", "wpb": "301.1", "bsz": "15.9", "num_updates": "8600", "lr": "4.95948e-05", "gnorm": "2.728", "train_wall": "31", "wall": "4437"}
2021-10-24 21:35:24 | INFO | train_inner | {"epoch": 8, "update": 7.102, "loss": "2.239", "nll_loss": "0.298", "ppl": "1.23", "wps": "914.8", "ups": "3.4", "wpb": "268.8", "bsz": "16", "num_updates": "8700", "lr": "4.95898e-05", "gnorm": "2.345", "train_wall": "29", "wall": "4467"}
2021-10-24 21:35:55 | INFO | train_inner | {"epoch": 8, "update": 7.184, "loss": "2.247", "nll_loss": "0.31", "ppl": "1.24", "wps": "1042.2", "ups": "3.18", "wpb": "327.7", "bsz": "16", "num_updates": "8800", "lr": "4.95848e-05", "gnorm": "2.616", "train_wall": "31", "wall": "4498"}
2021-10-24 21:36:28 | INFO | train_inner | {"epoch": 8, "update": 7.265, "loss": "2.246", "nll_loss": "0.307", "ppl": "1.24", "wps": "983.8", "ups": "3.09", "wpb": "318.2", "bsz": "16", "num_updates": "8900", "lr": "4.95798e-05", "gnorm": "2.654", "train_wall": "32", "wall": "4531"}
2021-10-24 21:36:58 | INFO | train_inner | {"epoch": 8, "update": 7.347, "loss": "2.254", "nll_loss": "0.317", "ppl": "1.25", "wps": "1104", "ups": "3.28", "wpb": "336.8", "bsz": "16", "num_updates": "9000", "lr": "4.95748e-05", "gnorm": "2.51", "train_wall": "30", "wall": "4561"}
2021-10-24 21:37:33 | INFO | train_inner | {"epoch": 8, "update": 7.429, "loss": "2.241", "nll_loss": "0.307", "ppl": "1.24", "wps": "834.4", "ups": "2.86", "wpb": "291.8", "bsz": "16", "num_updates": "9100", "lr": "4.95698e-05", "gnorm": "2.598", "train_wall": "35", "wall": "4596"}
2021-10-24 21:38:05 | INFO | train_inner | {"epoch": 8, "update": 7.51, "loss": "2.27", "nll_loss": "0.329", "ppl": "1.26", "wps": "1065.6", "ups": "3.11", "wpb": "342.9", "bsz": "16", "num_updates": "9200", "lr": "4.95648e-05", "gnorm": "2.848", "train_wall": "32", "wall": "4628"}
2021-10-24 21:38:39 | INFO | train_inner | {"epoch": 8, "update": 7.592, "loss": "2.264", "nll_loss": "0.334", "ppl": "1.26", "wps": "855.3", "ups": "3.01", "wpb": "284.1", "bsz": "16", "num_updates": "9300", "lr": "4.95598e-05", "gnorm": "2.623", "train_wall": "33", "wall": "4661"}
2021-10-24 21:39:16 | INFO | train_inner | {"epoch": 8, "update": 7.673, "loss": "2.267", "nll_loss": "0.332", "ppl": "1.26", "wps": "732.8", "ups": "2.7", "wpb": "270.9", "bsz": "16", "num_updates": "9400", "lr": "4.95548e-05", "gnorm": "2.793", "train_wall": "37", "wall": "4698"}
2021-10-24 21:39:54 | INFO | train_inner | {"epoch": 8, "update": 7.755, "loss": "2.266", "nll_loss": "0.323", "ppl": "1.25", "wps": "755.9", "ups": "2.6", "wpb": "291.1", "bsz": "16", "num_updates": "9500", "lr": "4.95498e-05", "gnorm": "2.63", "train_wall": "38", "wall": "4737"}
2021-10-24 21:40:30 | INFO | train_inner | {"epoch": 8, "update": 7.837, "loss": "2.263", "nll_loss": "0.33", "ppl": "1.26", "wps": "838.2", "ups": "2.77", "wpb": "302.9", "bsz": "16", "num_updates": "9600", "lr": "4.95448e-05", "gnorm": "2.847", "train_wall": "36", "wall": "4773"}
2021-10-24 21:41:07 | INFO | train_inner | {"epoch": 8, "update": 7.918, "loss": "2.251", "nll_loss": "0.322", "ppl": "1.25", "wps": "1066.3", "ups": "2.71", "wpb": "393.4", "bsz": "16", "num_updates": "9700", "lr": "4.95398e-05", "gnorm": "2.914", "train_wall": "37", "wall": "4810"}
2021-10-24 21:41:42 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 21:41:42 | INFO | train_inner | {"epoch": 8, "update": 8.0, "loss": "2.27", "nll_loss": "0.336", "ppl": "1.26", "wps": "861.8", "ups": "2.86", "wpb": "301.3", "bsz": "15.9", "num_updates": "9800", "lr": "4.95348e-05", "gnorm": "2.596", "train_wall": "35", "wall": "4845"}
2021-10-24 21:41:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 21:44:53 | INFO | valid | {"epoch": 8, "valid_loss": "2.972", "valid_nll_loss": "0.964", "valid_ppl": "1.95", "valid_bleu": "62.87", "valid_wps": "236.7", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "9800", "valid_best_bleu": "65.74"}
2021-10-24 21:44:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 21:45:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 8 @ 9800 updates, score 62.87) (writing took 6.803596434067003 seconds)
2021-10-24 21:45:00 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-10-24 21:45:00 | INFO | train | {"epoch": 8, "train_loss": "2.256", "train_nll_loss": "0.32", "train_ppl": "1.25", "train_wps": "618.1", "train_ups": "1.99", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "9800", "train_lr": "4.95348e-05", "train_gnorm": "2.657", "train_train_wall": "414", "train_wall": "5043"}
2021-10-24 21:45:00 | INFO | fairseq_cli.train | begin training epoch 8
2021-10-24 21:45:33 | INFO | train_inner | {"epoch": 9, "update": 8.082, "loss": "2.201", "nll_loss": "0.26", "ppl": "1.2", "wps": "140.2", "ups": "0.43", "wpb": "323.9", "bsz": "16", "num_updates": "9900", "lr": "4.95298e-05", "gnorm": "2.1", "train_wall": "32", "wall": "5076"}
2021-10-24 21:46:08 | INFO | train_inner | {"epoch": 9, "update": 8.163, "loss": "2.216", "nll_loss": "0.276", "ppl": "1.21", "wps": "861.8", "ups": "2.88", "wpb": "299.6", "bsz": "16", "num_updates": "10000", "lr": "4.95248e-05", "gnorm": "2.181", "train_wall": "35", "wall": "5111"}
2021-10-24 21:46:43 | INFO | train_inner | {"epoch": 9, "update": 8.245, "loss": "2.216", "nll_loss": "0.279", "ppl": "1.21", "wps": "870.1", "ups": "2.83", "wpb": "307.9", "bsz": "16", "num_updates": "10100", "lr": "4.95198e-05", "gnorm": "2.529", "train_wall": "35", "wall": "5146"}
2021-10-24 21:47:17 | INFO | train_inner | {"epoch": 9, "update": 8.327, "loss": "2.236", "nll_loss": "0.303", "ppl": "1.23", "wps": "906.4", "ups": "2.97", "wpb": "305.1", "bsz": "16", "num_updates": "10200", "lr": "4.95148e-05", "gnorm": "2.504", "train_wall": "33", "wall": "5180"}
2021-10-24 21:47:49 | INFO | train_inner | {"epoch": 9, "update": 8.408, "loss": "2.243", "nll_loss": "0.308", "ppl": "1.24", "wps": "983.2", "ups": "3.08", "wpb": "318.8", "bsz": "16", "num_updates": "10300", "lr": "4.95098e-05", "gnorm": "2.481", "train_wall": "32", "wall": "5212"}
2021-10-24 21:48:24 | INFO | train_inner | {"epoch": 9, "update": 8.49, "loss": "2.223", "nll_loss": "0.288", "ppl": "1.22", "wps": "925", "ups": "2.87", "wpb": "321.8", "bsz": "16", "num_updates": "10400", "lr": "4.95048e-05", "gnorm": "2.526", "train_wall": "35", "wall": "5247"}
2021-10-24 21:48:57 | INFO | train_inner | {"epoch": 9, "update": 8.571, "loss": "2.23", "nll_loss": "0.294", "ppl": "1.23", "wps": "981", "ups": "3.02", "wpb": "325.2", "bsz": "16", "num_updates": "10500", "lr": "4.94997e-05", "gnorm": "2.503", "train_wall": "33", "wall": "5280"}
2021-10-24 21:49:33 | INFO | train_inner | {"epoch": 9, "update": 8.653, "loss": "2.238", "nll_loss": "0.305", "ppl": "1.24", "wps": "852.5", "ups": "2.83", "wpb": "301.2", "bsz": "16", "num_updates": "10600", "lr": "4.94947e-05", "gnorm": "2.668", "train_wall": "35", "wall": "5315"}
2021-10-24 21:50:07 | INFO | train_inner | {"epoch": 9, "update": 8.735, "loss": "2.239", "nll_loss": "0.308", "ppl": "1.24", "wps": "850.5", "ups": "2.94", "wpb": "289.6", "bsz": "16", "num_updates": "10700", "lr": "4.94897e-05", "gnorm": "2.452", "train_wall": "34", "wall": "5349"}
2021-10-24 21:50:42 | INFO | train_inner | {"epoch": 9, "update": 8.816, "loss": "2.261", "nll_loss": "0.329", "ppl": "1.26", "wps": "799.6", "ups": "2.87", "wpb": "279.1", "bsz": "16", "num_updates": "10800", "lr": "4.94847e-05", "gnorm": "2.992", "train_wall": "35", "wall": "5384"}
2021-10-24 21:51:17 | INFO | train_inner | {"epoch": 9, "update": 8.898, "loss": "2.254", "nll_loss": "0.321", "ppl": "1.25", "wps": "772.9", "ups": "2.79", "wpb": "277", "bsz": "16", "num_updates": "10900", "lr": "4.94797e-05", "gnorm": "2.601", "train_wall": "36", "wall": "5420"}
2021-10-24 21:51:50 | INFO | train_inner | {"epoch": 9, "update": 8.98, "loss": "2.245", "nll_loss": "0.316", "ppl": "1.24", "wps": "1119.4", "ups": "3.02", "wpb": "370.4", "bsz": "16", "num_updates": "11000", "lr": "4.94747e-05", "gnorm": "2.475", "train_wall": "33", "wall": "5453"}
2021-10-24 21:52:00 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 21:52:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 21:55:18 | INFO | valid | {"epoch": 9, "valid_loss": "3.006", "valid_nll_loss": "1.007", "valid_ppl": "2.01", "valid_bleu": "63.39", "valid_wps": "228.1", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "11025", "valid_best_bleu": "65.74"}
2021-10-24 21:55:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 21:55:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 9 @ 11025 updates, score 63.39) (writing took 24.897706936928444 seconds)
2021-10-24 21:55:43 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-10-24 21:55:43 | INFO | train | {"epoch": 9, "train_loss": "2.234", "train_nll_loss": "0.299", "train_ppl": "1.23", "train_wps": "591.1", "train_ups": "1.9", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "11025", "train_lr": "4.94735e-05", "train_gnorm": "2.496", "train_train_wall": "416", "train_wall": "5686"}
2021-10-24 21:55:43 | INFO | fairseq_cli.train | begin training epoch 9
2021-10-24 21:56:11 | INFO | train_inner | {"epoch": 10, "update": 9.061, "loss": "2.21", "nll_loss": "0.275", "ppl": "1.21", "wps": "113.1", "ups": "0.38", "wpb": "294.1", "bsz": "15.9", "num_updates": "11100", "lr": "4.94697e-05", "gnorm": "2.136", "train_wall": "36", "wall": "5713"}
2021-10-24 21:56:46 | INFO | train_inner | {"epoch": 10, "update": 9.143, "loss": "2.21", "nll_loss": "0.274", "ppl": "1.21", "wps": "830.1", "ups": "2.85", "wpb": "291", "bsz": "16", "num_updates": "11200", "lr": "4.94647e-05", "gnorm": "2.624", "train_wall": "35", "wall": "5748"}
2021-10-24 21:57:21 | INFO | train_inner | {"epoch": 10, "update": 9.224, "loss": "2.213", "nll_loss": "0.274", "ppl": "1.21", "wps": "873.4", "ups": "2.84", "wpb": "308", "bsz": "16", "num_updates": "11300", "lr": "4.94597e-05", "gnorm": "2.238", "train_wall": "35", "wall": "5784"}
2021-10-24 21:57:59 | INFO | train_inner | {"epoch": 10, "update": 9.306, "loss": "2.2", "nll_loss": "0.269", "ppl": "1.2", "wps": "803.8", "ups": "2.63", "wpb": "305.2", "bsz": "16", "num_updates": "11400", "lr": "4.94547e-05", "gnorm": "2.225", "train_wall": "38", "wall": "5822"}
2021-10-24 21:58:36 | INFO | train_inner | {"epoch": 10, "update": 9.388, "loss": "2.209", "nll_loss": "0.275", "ppl": "1.21", "wps": "757.9", "ups": "2.67", "wpb": "284", "bsz": "16", "num_updates": "11500", "lr": "4.94497e-05", "gnorm": "2.387", "train_wall": "37", "wall": "5859"}
2021-10-24 21:59:09 | INFO | train_inner | {"epoch": 10, "update": 9.469, "loss": "2.211", "nll_loss": "0.279", "ppl": "1.21", "wps": "1041.1", "ups": "3.02", "wpb": "345", "bsz": "16", "num_updates": "11600", "lr": "4.94447e-05", "gnorm": "2.354", "train_wall": "33", "wall": "5892"}
2021-10-24 21:59:45 | INFO | train_inner | {"epoch": 10, "update": 9.551, "loss": "2.229", "nll_loss": "0.297", "ppl": "1.23", "wps": "894.2", "ups": "2.78", "wpb": "321.8", "bsz": "16", "num_updates": "11700", "lr": "4.94397e-05", "gnorm": "2.511", "train_wall": "36", "wall": "5928"}
2021-10-24 22:00:20 | INFO | train_inner | {"epoch": 10, "update": 9.633, "loss": "2.217", "nll_loss": "0.288", "ppl": "1.22", "wps": "945.8", "ups": "2.87", "wpb": "330", "bsz": "16", "num_updates": "11800", "lr": "4.94347e-05", "gnorm": "2.354", "train_wall": "35", "wall": "5963"}
2021-10-24 22:00:54 | INFO | train_inner | {"epoch": 10, "update": 9.714, "loss": "2.239", "nll_loss": "0.31", "ppl": "1.24", "wps": "958.9", "ups": "2.98", "wpb": "321.9", "bsz": "16", "num_updates": "11900", "lr": "4.94297e-05", "gnorm": "2.639", "train_wall": "33", "wall": "5997"}
2021-10-24 22:01:27 | INFO | train_inner | {"epoch": 10, "update": 9.796, "loss": "2.239", "nll_loss": "0.308", "ppl": "1.24", "wps": "777.8", "ups": "3.06", "wpb": "254.3", "bsz": "16", "num_updates": "12000", "lr": "4.94247e-05", "gnorm": "2.781", "train_wall": "32", "wall": "6029"}
2021-10-24 22:02:03 | INFO | train_inner | {"epoch": 10, "update": 9.878, "loss": "2.253", "nll_loss": "0.324", "ppl": "1.25", "wps": "930.6", "ups": "2.72", "wpb": "342.7", "bsz": "16", "num_updates": "12100", "lr": "4.94197e-05", "gnorm": "2.632", "train_wall": "37", "wall": "6066"}
2021-10-24 22:02:38 | INFO | train_inner | {"epoch": 10, "update": 9.959, "loss": "2.23", "nll_loss": "0.302", "ppl": "1.23", "wps": "896.5", "ups": "2.86", "wpb": "313.7", "bsz": "16", "num_updates": "12200", "lr": "4.94147e-05", "gnorm": "2.732", "train_wall": "35", "wall": "6101"}
2021-10-24 22:02:56 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 22:02:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 22:06:09 | INFO | valid | {"epoch": 10, "valid_loss": "3.017", "valid_nll_loss": "1.037", "valid_ppl": "2.05", "valid_bleu": "62.22", "valid_wps": "234.2", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "12250", "valid_best_bleu": "65.74"}
2021-10-24 22:06:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 22:06:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 10 @ 12250 updates, score 62.22) (writing took 7.639370232005604 seconds)
2021-10-24 22:06:17 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-10-24 22:06:17 | INFO | train | {"epoch": 10, "train_loss": "2.221", "train_nll_loss": "0.289", "train_ppl": "1.22", "train_wps": "600.2", "train_ups": "1.93", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "12250", "train_lr": "4.94122e-05", "train_gnorm": "2.464", "train_train_wall": "429", "train_wall": "6320"}
2021-10-24 22:06:17 | INFO | fairseq_cli.train | begin training epoch 10
2021-10-24 22:06:34 | INFO | train_inner | {"epoch": 11, "update": 10.041, "loss": "2.196", "nll_loss": "0.263", "ppl": "1.2", "wps": "130.2", "ups": "0.42", "wpb": "306.4", "bsz": "15.9", "num_updates": "12300", "lr": "4.94097e-05", "gnorm": "2.106", "train_wall": "34", "wall": "6337"}
2021-10-24 22:07:09 | INFO | train_inner | {"epoch": 11, "update": 10.122, "loss": "2.216", "nll_loss": "0.283", "ppl": "1.22", "wps": "897.5", "ups": "2.83", "wpb": "317", "bsz": "16", "num_updates": "12400", "lr": "4.94047e-05", "gnorm": "2.499", "train_wall": "35", "wall": "6372"}
2021-10-24 22:07:42 | INFO | train_inner | {"epoch": 11, "update": 10.204, "loss": "2.184", "nll_loss": "0.251", "ppl": "1.19", "wps": "938.5", "ups": "3.08", "wpb": "305", "bsz": "16", "num_updates": "12500", "lr": "4.93997e-05", "gnorm": "2.178", "train_wall": "32", "wall": "6404"}
2021-10-24 22:08:18 | INFO | train_inner | {"epoch": 11, "update": 10.286, "loss": "2.188", "nll_loss": "0.257", "ppl": "1.2", "wps": "848.3", "ups": "2.76", "wpb": "307.3", "bsz": "16", "num_updates": "12600", "lr": "4.93947e-05", "gnorm": "2.315", "train_wall": "36", "wall": "6441"}
2021-10-24 22:08:55 | INFO | train_inner | {"epoch": 11, "update": 10.367, "loss": "2.226", "nll_loss": "0.297", "ppl": "1.23", "wps": "858.2", "ups": "2.72", "wpb": "315.4", "bsz": "16", "num_updates": "12700", "lr": "4.93897e-05", "gnorm": "2.551", "train_wall": "36", "wall": "6477"}
2021-10-24 22:09:28 | INFO | train_inner | {"epoch": 11, "update": 10.449, "loss": "2.219", "nll_loss": "0.285", "ppl": "1.22", "wps": "881.5", "ups": "2.99", "wpb": "294.9", "bsz": "16", "num_updates": "12800", "lr": "4.93847e-05", "gnorm": "2.453", "train_wall": "33", "wall": "6511"}
2021-10-24 22:10:05 | INFO | train_inner | {"epoch": 11, "update": 10.531, "loss": "2.221", "nll_loss": "0.293", "ppl": "1.23", "wps": "775.6", "ups": "2.68", "wpb": "288.9", "bsz": "16", "num_updates": "12900", "lr": "4.93797e-05", "gnorm": "3.507", "train_wall": "37", "wall": "6548"}
2021-10-24 22:10:40 | INFO | train_inner | {"epoch": 11, "update": 10.612, "loss": "2.216", "nll_loss": "0.286", "ppl": "1.22", "wps": "1009.1", "ups": "2.88", "wpb": "351", "bsz": "16", "num_updates": "13000", "lr": "4.93747e-05", "gnorm": "2.559", "train_wall": "35", "wall": "6583"}
2021-10-24 22:11:13 | INFO | train_inner | {"epoch": 11, "update": 10.694, "loss": "2.207", "nll_loss": "0.278", "ppl": "1.21", "wps": "968.1", "ups": "2.99", "wpb": "323.3", "bsz": "16", "num_updates": "13100", "lr": "4.93697e-05", "gnorm": "2.418", "train_wall": "33", "wall": "6616"}
2021-10-24 22:11:50 | INFO | train_inner | {"epoch": 11, "update": 10.776, "loss": "2.217", "nll_loss": "0.29", "ppl": "1.22", "wps": "875.7", "ups": "2.77", "wpb": "315.6", "bsz": "16", "num_updates": "13200", "lr": "4.93647e-05", "gnorm": "2.433", "train_wall": "36", "wall": "6652"}
2021-10-24 22:12:22 | INFO | train_inner | {"epoch": 11, "update": 10.857, "loss": "2.216", "nll_loss": "0.288", "ppl": "1.22", "wps": "881", "ups": "3.1", "wpb": "284.6", "bsz": "16", "num_updates": "13300", "lr": "4.93597e-05", "gnorm": "2.403", "train_wall": "32", "wall": "6685"}
2021-10-24 22:12:55 | INFO | train_inner | {"epoch": 11, "update": 10.939, "loss": "2.212", "nll_loss": "0.288", "ppl": "1.22", "wps": "933.4", "ups": "3", "wpb": "311.3", "bsz": "16", "num_updates": "13400", "lr": "4.93547e-05", "gnorm": "2.492", "train_wall": "33", "wall": "6718"}
2021-10-24 22:13:21 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 22:13:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 22:16:41 | INFO | valid | {"epoch": 11, "valid_loss": "3.035", "valid_nll_loss": "1.074", "valid_ppl": "2.11", "valid_bleu": "60.46", "valid_wps": "226.7", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "13475", "valid_best_bleu": "65.74"}
2021-10-24 22:16:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 22:16:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 11 @ 13475 updates, score 60.46) (writing took 7.231557096005417 seconds)
2021-10-24 22:16:48 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-10-24 22:16:48 | INFO | train | {"epoch": 11, "train_loss": "2.21", "train_nll_loss": "0.28", "train_ppl": "1.21", "train_wps": "602.3", "train_ups": "1.94", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "13475", "train_lr": "4.93509e-05", "train_gnorm": "2.507", "train_train_wall": "421", "train_wall": "6951"}
2021-10-24 22:16:48 | INFO | fairseq_cli.train | begin training epoch 11
2021-10-24 22:16:56 | INFO | train_inner | {"epoch": 12, "update": 11.02, "loss": "2.208", "nll_loss": "0.278", "ppl": "1.21", "wps": "133.2", "ups": "0.41", "wpb": "321.3", "bsz": "15.9", "num_updates": "13500", "lr": "4.93497e-05", "gnorm": "2.504", "train_wall": "34", "wall": "6959"}
2021-10-24 22:17:29 | INFO | train_inner | {"epoch": 12, "update": 11.102, "loss": "2.189", "nll_loss": "0.258", "ppl": "1.2", "wps": "1100.3", "ups": "3.09", "wpb": "356.3", "bsz": "16", "num_updates": "13600", "lr": "4.93447e-05", "gnorm": "2.126", "train_wall": "32", "wall": "6992"}
2021-10-24 22:18:04 | INFO | train_inner | {"epoch": 12, "update": 11.184, "loss": "2.203", "nll_loss": "0.277", "ppl": "1.21", "wps": "907.1", "ups": "2.85", "wpb": "318", "bsz": "16", "num_updates": "13700", "lr": "4.93397e-05", "gnorm": "2.3", "train_wall": "35", "wall": "7027"}
2021-10-24 22:18:38 | INFO | train_inner | {"epoch": 12, "update": 11.265, "loss": "2.208", "nll_loss": "0.281", "ppl": "1.21", "wps": "880.7", "ups": "2.94", "wpb": "299.6", "bsz": "16", "num_updates": "13800", "lr": "4.93347e-05", "gnorm": "2.277", "train_wall": "34", "wall": "7061"}
2021-10-24 22:19:16 | INFO | train_inner | {"epoch": 12, "update": 11.347, "loss": "2.195", "nll_loss": "0.265", "ppl": "1.2", "wps": "742.8", "ups": "2.64", "wpb": "281.2", "bsz": "16", "num_updates": "13900", "lr": "4.93297e-05", "gnorm": "2.279", "train_wall": "38", "wall": "7099"}
2021-10-24 22:19:52 | INFO | train_inner | {"epoch": 12, "update": 11.429, "loss": "2.202", "nll_loss": "0.276", "ppl": "1.21", "wps": "829.5", "ups": "2.79", "wpb": "296.8", "bsz": "16", "num_updates": "14000", "lr": "4.93247e-05", "gnorm": "2.554", "train_wall": "36", "wall": "7134"}
2021-10-24 22:20:25 | INFO | train_inner | {"epoch": 12, "update": 11.51, "loss": "2.184", "nll_loss": "0.256", "ppl": "1.19", "wps": "941.1", "ups": "3", "wpb": "313.3", "bsz": "16", "num_updates": "14100", "lr": "4.93197e-05", "gnorm": "2.18", "train_wall": "33", "wall": "7168"}
2021-10-24 22:21:02 | INFO | train_inner | {"epoch": 12, "update": 11.592, "loss": "2.201", "nll_loss": "0.272", "ppl": "1.21", "wps": "826.4", "ups": "2.71", "wpb": "305.4", "bsz": "16", "num_updates": "14200", "lr": "4.93147e-05", "gnorm": "2.377", "train_wall": "37", "wall": "7205"}
2021-10-24 22:21:31 | INFO | train_inner | {"epoch": 12, "update": 11.673, "loss": "2.203", "nll_loss": "0.276", "ppl": "1.21", "wps": "989.7", "ups": "3.45", "wpb": "287.2", "bsz": "16", "num_updates": "14300", "lr": "4.93097e-05", "gnorm": "2.22", "train_wall": "29", "wall": "7234"}
2021-10-24 22:22:02 | INFO | train_inner | {"epoch": 12, "update": 11.755, "loss": "2.214", "nll_loss": "0.284", "ppl": "1.22", "wps": "916.2", "ups": "3.17", "wpb": "289.2", "bsz": "16", "num_updates": "14400", "lr": "4.93047e-05", "gnorm": "2.361", "train_wall": "31", "wall": "7265"}
2021-10-24 22:22:36 | INFO | train_inner | {"epoch": 12, "update": 11.837, "loss": "2.205", "nll_loss": "0.278", "ppl": "1.21", "wps": "961.2", "ups": "2.99", "wpb": "321.3", "bsz": "16", "num_updates": "14500", "lr": "4.92996e-05", "gnorm": "2.376", "train_wall": "33", "wall": "7299"}
2021-10-24 22:23:08 | INFO | train_inner | {"epoch": 12, "update": 11.918, "loss": "2.226", "nll_loss": "0.304", "ppl": "1.23", "wps": "1038.6", "ups": "3.12", "wpb": "332.7", "bsz": "16", "num_updates": "14600", "lr": "4.92946e-05", "gnorm": "2.429", "train_wall": "32", "wall": "7331"}
2021-10-24 22:23:43 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 22:23:43 | INFO | train_inner | {"epoch": 12, "update": 12.0, "loss": "2.235", "nll_loss": "0.315", "ppl": "1.24", "wps": "938.1", "ups": "2.83", "wpb": "331.9", "bsz": "15.9", "num_updates": "14700", "lr": "4.92896e-05", "gnorm": "2.332", "train_wall": "35", "wall": "7366"}
2021-10-24 22:23:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 22:27:04 | INFO | valid | {"epoch": 12, "valid_loss": "3.062", "valid_nll_loss": "1.09", "valid_ppl": "2.13", "valid_bleu": "57.79", "valid_wps": "226.1", "valid_wpb": "73.8", "valid_bsz": "4", "valid_num_updates": "14700", "valid_best_bleu": "65.74"}
2021-10-24 22:27:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 22:27:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 12 @ 14700 updates, score 57.79) (writing took 7.024736849940382 seconds)
2021-10-24 22:27:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-10-24 22:27:11 | INFO | train | {"epoch": 12, "train_loss": "2.205", "train_nll_loss": "0.278", "train_ppl": "1.21", "train_wps": "610.6", "train_ups": "1.97", "train_wpb": "310.3", "train_bsz": "16", "train_num_updates": "14700", "train_lr": "4.92896e-05", "train_gnorm": "2.318", "train_train_wall": "412", "train_wall": "7573"}
2021-10-24 22:27:11 | INFO | fairseq_cli.train | begin training epoch 12
2021-10-24 22:27:48 | INFO | train_inner | {"epoch": 13, "update": 12.082, "loss": "2.181", "nll_loss": "0.252", "ppl": "1.19", "wps": "129", "ups": "0.41", "wpb": "315.7", "bsz": "16", "num_updates": "14800", "lr": "4.92846e-05", "gnorm": "2.317", "train_wall": "37", "wall": "7611"}
2021-10-24 22:28:23 | INFO | train_inner | {"epoch": 13, "update": 12.163, "loss": "2.188", "nll_loss": "0.259", "ppl": "1.2", "wps": "807.4", "ups": "2.85", "wpb": "283.1", "bsz": "16", "num_updates": "14900", "lr": "4.92796e-05", "gnorm": "2.151", "train_wall": "35", "wall": "7646"}
run.sh: line 186: UTH_PATH: command not found
run.sh: line 188: syntax error near unexpected token `}'
run.sh: line 188: `}'
Source: source Target: target
2021-10-24 22:42:37 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.1, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../processed_data/large/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eval_bleu=True, eval_bleu_args='{"beam": 5}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, langs='java,python,en_XX', layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='json', log_interval=100, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=3000, max_sentences=4, max_sentences_valid=4, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=200000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_workers=40, optimizer='adam', optimizer_overrides='{}', patience=10, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='../plbart/checkpoint_11_100000.pt', save_dir='../models/plbart/unique/large', save_interval=1, save_interval_updates=0, seed=1234, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='source', stop_time_hours=0, target_lang='target', task='translation_without_lang_token', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', truncate_source=True, update_freq=[4], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir='../user_dir', valid_subset='valid', validate_interval=1, warmup_updates=500, weight_decay=0.0)
2021-10-24 22:42:37 | INFO | fairseq.tasks.translation | [source] dictionary: 50001 types
2021-10-24 22:42:37 | INFO | fairseq.tasks.translation | [target] dictionary: 50001 types
2021-10-24 22:42:37 | INFO | fairseq.data.data_utils | loaded 2449 examples from: ../processed_data/large/data-bin/valid.source-target.source
2021-10-24 22:42:37 | INFO | fairseq.data.data_utils | loaded 2449 examples from: ../processed_data/large/data-bin/valid.source-target.target
2021-10-24 22:42:37 | INFO | fairseq.tasks.translation | ../processed_data/large/data-bin valid source-target 2449 examples
2021-10-24 22:42:42 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-10-24 22:42:42 | INFO | fairseq_cli.train | model mbart_base, criterion LabelSmoothedCrossEntropyCriterion
2021-10-24 22:42:42 | INFO | fairseq_cli.train | num. model params: 139220736 (num. trained: 139220736)
2021-10-24 22:42:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-10-24 22:42:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-10-24 22:42:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-10-24 22:42:45 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 12.000 GB ; name = GRID P40-12Q                            
2021-10-24 22:42:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-10-24 22:42:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-10-24 22:42:45 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 4
Source: source Target: target
2021-10-24 22:43:18 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.1, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../processed_data/large/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eval_bleu=True, eval_bleu_args='{"beam": 5}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, langs='java,python,en_XX', layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='json', log_interval=100, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=3000, max_sentences=4, max_sentences_valid=4, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=200000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_workers=40, optimizer='adam', optimizer_overrides='{}', patience=10, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='../plbart/checkpoint_11_100000.pt', save_dir='../models/plbart/unique/large', save_interval=1, save_interval_updates=0, seed=1234, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='source', stop_time_hours=0, target_lang='target', task='translation_without_lang_token', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', truncate_source=True, update_freq=[4], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir='../user_dir', valid_subset='valid', validate_interval=1, warmup_updates=500, weight_decay=0.0)
2021-10-24 22:43:18 | INFO | fairseq.tasks.translation | [source] dictionary: 50001 types
2021-10-24 22:43:18 | INFO | fairseq.tasks.translation | [target] dictionary: 50001 types
2021-10-24 22:43:18 | INFO | fairseq.data.data_utils | loaded 2449 examples from: ../processed_data/large/data-bin/valid.source-target.source
2021-10-24 22:43:18 | INFO | fairseq.data.data_utils | loaded 2449 examples from: ../processed_data/large/data-bin/valid.source-target.target
2021-10-24 22:43:18 | INFO | fairseq.tasks.translation | ../processed_data/large/data-bin valid source-target 2449 examples
2021-10-24 22:43:23 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50005, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=50005, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-10-24 22:43:23 | INFO | fairseq_cli.train | model mbart_base, criterion LabelSmoothedCrossEntropyCriterion
2021-10-24 22:43:23 | INFO | fairseq_cli.train | num. model params: 139220736 (num. trained: 139220736)
2021-10-24 22:43:26 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-10-24 22:43:26 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-10-24 22:43:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-10-24 22:43:26 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 12.000 GB ; name = GRID P40-12Q                            
2021-10-24 22:43:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-10-24 22:43:26 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-10-24 22:43:26 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 4
2021-10-24 22:43:29 | INFO | fairseq.trainer | loaded checkpoint ../plbart/checkpoint_11_100000.pt (epoch 11 @ 0 updates)
2021-10-24 22:43:29 | INFO | fairseq.optim.adam | using FusedAdam
2021-10-24 22:43:29 | INFO | fairseq.trainer | loading train data for epoch 1
2021-10-24 22:43:29 | INFO | fairseq.data.data_utils | loaded 19590 examples from: ../processed_data/large/data-bin/train.source-target.source
2021-10-24 22:43:29 | INFO | fairseq.data.data_utils | loaded 19590 examples from: ../processed_data/large/data-bin/train.source-target.target
2021-10-24 22:43:29 | INFO | fairseq.tasks.translation | ../processed_data/large/data-bin train source-target 19590 examples
2021-10-24 22:43:29 | INFO | fairseq_cli.train | begin training epoch 1
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/mahim/miniconda3/envs/python36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2021-10-24 22:44:05 | INFO | train_inner | {"epoch": 1, "update": 0.082, "loss": "6.641", "nll_loss": "4.616", "ppl": "24.52", "wps": "982.5", "ups": "2.95", "wpb": "332.6", "bsz": "16", "num_updates": "100", "lr": "1e-05", "gnorm": "51.682", "train_wall": "34", "wall": "39"}
2021-10-24 22:44:42 | INFO | train_inner | {"epoch": 1, "update": 0.163, "loss": "4.026", "nll_loss": "2.04", "ppl": "4.11", "wps": "760.9", "ups": "2.68", "wpb": "284.3", "bsz": "16", "num_updates": "200", "lr": "2e-05", "gnorm": "11.053", "train_wall": "37", "wall": "76"}
2021-10-24 22:45:21 | INFO | train_inner | {"epoch": 1, "update": 0.245, "loss": "3.26", "nll_loss": "1.265", "ppl": "2.4", "wps": "818.2", "ups": "2.59", "wpb": "316.5", "bsz": "16", "num_updates": "300", "lr": "3e-05", "gnorm": "5.731", "train_wall": "38", "wall": "115"}
2021-10-24 22:45:58 | INFO | train_inner | {"epoch": 1, "update": 0.327, "loss": "3.277", "nll_loss": "1.354", "ppl": "2.56", "wps": "681.8", "ups": "2.67", "wpb": "255.5", "bsz": "16", "num_updates": "400", "lr": "4e-05", "gnorm": "4.35", "train_wall": "37", "wall": "153"}
2021-10-24 22:46:31 | INFO | train_inner | {"epoch": 1, "update": 0.408, "loss": "2.982", "nll_loss": "1.041", "ppl": "2.06", "wps": "1036.3", "ups": "3.04", "wpb": "341.2", "bsz": "16", "num_updates": "500", "lr": "5e-05", "gnorm": "3.924", "train_wall": "33", "wall": "185"}
2021-10-24 22:47:04 | INFO | train_inner | {"epoch": 1, "update": 0.49, "loss": "3.039", "nll_loss": "1.126", "ppl": "2.18", "wps": "878.6", "ups": "3.05", "wpb": "288.4", "bsz": "16", "num_updates": "600", "lr": "4.9995e-05", "gnorm": "3.86", "train_wall": "33", "wall": "218"}
2021-10-24 22:47:41 | INFO | train_inner | {"epoch": 1, "update": 0.571, "loss": "2.816", "nll_loss": "0.88", "ppl": "1.84", "wps": "957.3", "ups": "2.7", "wpb": "355.1", "bsz": "16", "num_updates": "700", "lr": "4.999e-05", "gnorm": "3.307", "train_wall": "37", "wall": "255"}
2021-10-24 22:48:18 | INFO | train_inner | {"epoch": 1, "update": 0.653, "loss": "2.894", "nll_loss": "0.974", "ppl": "1.96", "wps": "891.3", "ups": "2.74", "wpb": "325.1", "bsz": "16", "num_updates": "800", "lr": "4.9985e-05", "gnorm": "3.409", "train_wall": "36", "wall": "292"}
2021-10-24 22:48:56 | INFO | train_inner | {"epoch": 1, "update": 0.735, "loss": "2.895", "nll_loss": "0.983", "ppl": "1.98", "wps": "816.7", "ups": "2.6", "wpb": "314.1", "bsz": "16", "num_updates": "900", "lr": "4.998e-05", "gnorm": "3.537", "train_wall": "38", "wall": "330"}
2021-10-24 22:49:31 | INFO | train_inner | {"epoch": 1, "update": 0.816, "loss": "3.032", "nll_loss": "1.146", "ppl": "2.21", "wps": "734.5", "ups": "2.84", "wpb": "258.5", "bsz": "16", "num_updates": "1000", "lr": "4.9975e-05", "gnorm": "3.36", "train_wall": "35", "wall": "366"}
2021-10-24 22:50:03 | INFO | train_inner | {"epoch": 1, "update": 0.898, "loss": "2.902", "nll_loss": "1.001", "ppl": "2", "wps": "957.4", "ups": "3.14", "wpb": "304.6", "bsz": "16", "num_updates": "1100", "lr": "4.997e-05", "gnorm": "3.139", "train_wall": "32", "wall": "397"}
2021-10-24 22:50:42 | INFO | train_inner | {"epoch": 1, "update": 0.98, "loss": "2.838", "nll_loss": "0.937", "ppl": "1.91", "wps": "780.1", "ups": "2.58", "wpb": "302.9", "bsz": "16", "num_updates": "1200", "lr": "4.9965e-05", "gnorm": "3.18", "train_wall": "39", "wall": "436"}
2021-10-24 22:50:51 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 22:50:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 22:54:02 | INFO | valid | {"epoch": 1, "valid_loss": "2.938", "valid_nll_loss": "0.869", "valid_ppl": "1.83", "valid_bleu": "64.8", "valid_wps": "235.5", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "1225"}
2021-10-24 22:54:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 22:54:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_best.pt (epoch 1 @ 1225 updates, score 64.8) (writing took 9.090050030965358 seconds)
2021-10-24 22:54:11 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-10-24 22:54:11 | INFO | train | {"epoch": 1, "train_loss": "3.389", "train_nll_loss": "1.452", "train_ppl": "2.74", "train_wps": "583.7", "train_ups": "1.91", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "1225", "train_lr": "4.99637e-05", "train_gnorm": "8.283", "train_train_wall": "437", "train_wall": "645"}
2021-10-24 22:54:11 | INFO | fairseq_cli.train | begin training epoch 1
2021-10-24 22:54:38 | INFO | train_inner | {"epoch": 2, "update": 1.061, "loss": "2.825", "nll_loss": "0.919", "ppl": "1.89", "wps": "116.8", "ups": "0.42", "wpb": "275.8", "bsz": "15.9", "num_updates": "1300", "lr": "4.996e-05", "gnorm": "3.342", "train_wall": "34", "wall": "672"}
2021-10-24 22:55:11 | INFO | train_inner | {"epoch": 2, "update": 1.143, "loss": "2.76", "nll_loss": "0.846", "ppl": "1.8", "wps": "919.4", "ups": "3.02", "wpb": "304.8", "bsz": "16", "num_updates": "1400", "lr": "4.9955e-05", "gnorm": "3.011", "train_wall": "33", "wall": "705"}
2021-10-24 22:55:49 | INFO | train_inner | {"epoch": 2, "update": 1.224, "loss": "2.75", "nll_loss": "0.841", "ppl": "1.79", "wps": "809", "ups": "2.63", "wpb": "307.3", "bsz": "16", "num_updates": "1500", "lr": "4.995e-05", "gnorm": "3.219", "train_wall": "38", "wall": "743"}
2021-10-24 22:56:27 | INFO | train_inner | {"epoch": 2, "update": 1.306, "loss": "2.735", "nll_loss": "0.825", "ppl": "1.77", "wps": "855.7", "ups": "2.67", "wpb": "320.7", "bsz": "16", "num_updates": "1600", "lr": "4.9945e-05", "gnorm": "2.926", "train_wall": "37", "wall": "781"}
2021-10-24 22:57:02 | INFO | train_inner | {"epoch": 2, "update": 1.388, "loss": "2.676", "nll_loss": "0.763", "ppl": "1.7", "wps": "871", "ups": "2.83", "wpb": "307.8", "bsz": "16", "num_updates": "1700", "lr": "4.994e-05", "gnorm": "2.722", "train_wall": "35", "wall": "816"}
2021-10-24 22:57:35 | INFO | train_inner | {"epoch": 2, "update": 1.469, "loss": "2.676", "nll_loss": "0.769", "ppl": "1.7", "wps": "921.5", "ups": "2.99", "wpb": "308.1", "bsz": "16", "num_updates": "1800", "lr": "4.9935e-05", "gnorm": "2.844", "train_wall": "33", "wall": "850"}
2021-10-24 22:58:11 | INFO | train_inner | {"epoch": 2, "update": 1.551, "loss": "2.727", "nll_loss": "0.823", "ppl": "1.77", "wps": "879.4", "ups": "2.83", "wpb": "311.1", "bsz": "16", "num_updates": "1900", "lr": "4.993e-05", "gnorm": "3.219", "train_wall": "35", "wall": "885"}
2021-10-24 22:58:44 | INFO | train_inner | {"epoch": 2, "update": 1.633, "loss": "2.725", "nll_loss": "0.822", "ppl": "1.77", "wps": "895.5", "ups": "3.03", "wpb": "295.5", "bsz": "16", "num_updates": "2000", "lr": "4.9925e-05", "gnorm": "3.171", "train_wall": "33", "wall": "918"}
2021-10-24 22:59:19 | INFO | train_inner | {"epoch": 2, "update": 1.714, "loss": "2.712", "nll_loss": "0.813", "ppl": "1.76", "wps": "897.5", "ups": "2.81", "wpb": "319.3", "bsz": "16", "num_updates": "2100", "lr": "4.992e-05", "gnorm": "3.02", "train_wall": "35", "wall": "954"}
2021-10-24 22:59:52 | INFO | train_inner | {"epoch": 2, "update": 1.796, "loss": "2.731", "nll_loss": "0.834", "ppl": "1.78", "wps": "935.5", "ups": "3.03", "wpb": "308.7", "bsz": "16", "num_updates": "2200", "lr": "4.9915e-05", "gnorm": "3.078", "train_wall": "33", "wall": "987"}
2021-10-24 23:00:30 | INFO | train_inner | {"epoch": 2, "update": 1.878, "loss": "2.69", "nll_loss": "0.788", "ppl": "1.73", "wps": "794.4", "ups": "2.67", "wpb": "297.7", "bsz": "16", "num_updates": "2300", "lr": "4.991e-05", "gnorm": "3.007", "train_wall": "37", "wall": "1024"}
2021-10-24 23:01:07 | INFO | train_inner | {"epoch": 2, "update": 1.959, "loss": "2.746", "nll_loss": "0.854", "ppl": "1.81", "wps": "825.1", "ups": "2.73", "wpb": "302.8", "bsz": "16", "num_updates": "2400", "lr": "4.9905e-05", "gnorm": "3.013", "train_wall": "36", "wall": "1061"}
2021-10-24 23:01:24 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 23:01:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 23:04:34 | INFO | valid | {"epoch": 2, "valid_loss": "2.856", "valid_nll_loss": "0.818", "valid_ppl": "1.76", "valid_bleu": "66.26", "valid_wps": "234.8", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "2450", "valid_best_bleu": "66.26"}
2021-10-24 23:04:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 23:04:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_best.pt (epoch 2 @ 2450 updates, score 66.26) (writing took 21.662133525009267 seconds)
2021-10-24 23:04:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-10-24 23:04:56 | INFO | train | {"epoch": 2, "train_loss": "2.726", "train_nll_loss": "0.821", "train_ppl": "1.77", "train_wps": "579", "train_ups": "1.9", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "2450", "train_lr": "4.99025e-05", "train_gnorm": "3.034", "train_train_wall": "428", "train_wall": "1290"}
2021-10-24 23:04:56 | INFO | fairseq_cli.train | begin training epoch 2
2021-10-24 23:05:16 | INFO | train_inner | {"epoch": 3, "update": 2.041, "loss": "2.604", "nll_loss": "0.69", "ppl": "1.61", "wps": "127.3", "ups": "0.4", "wpb": "317.9", "bsz": "15.9", "num_updates": "2500", "lr": "4.98999e-05", "gnorm": "2.831", "train_wall": "35", "wall": "1311"}
2021-10-24 23:05:54 | INFO | train_inner | {"epoch": 3, "update": 2.122, "loss": "2.588", "nll_loss": "0.667", "ppl": "1.59", "wps": "831.2", "ups": "2.67", "wpb": "311.6", "bsz": "16", "num_updates": "2600", "lr": "4.98949e-05", "gnorm": "3.034", "train_wall": "37", "wall": "1348"}
2021-10-24 23:06:32 | INFO | train_inner | {"epoch": 3, "update": 2.204, "loss": "2.575", "nll_loss": "0.659", "ppl": "1.58", "wps": "758", "ups": "2.6", "wpb": "291.9", "bsz": "16", "num_updates": "2700", "lr": "4.98899e-05", "gnorm": "3.075", "train_wall": "38", "wall": "1387"}
2021-10-24 23:07:10 | INFO | train_inner | {"epoch": 3, "update": 2.286, "loss": "2.584", "nll_loss": "0.67", "ppl": "1.59", "wps": "683.6", "ups": "2.63", "wpb": "259.9", "bsz": "16", "num_updates": "2800", "lr": "4.98849e-05", "gnorm": "3.21", "train_wall": "38", "wall": "1425"}
2021-10-24 23:07:47 | INFO | train_inner | {"epoch": 3, "update": 2.367, "loss": "2.546", "nll_loss": "0.626", "ppl": "1.54", "wps": "964.6", "ups": "2.76", "wpb": "349.3", "bsz": "16", "num_updates": "2900", "lr": "4.98799e-05", "gnorm": "3.048", "train_wall": "36", "wall": "1461"}
2021-10-24 23:08:25 | INFO | train_inner | {"epoch": 3, "update": 2.449, "loss": "2.545", "nll_loss": "0.63", "ppl": "1.55", "wps": "849.7", "ups": "2.6", "wpb": "327.1", "bsz": "16", "num_updates": "3000", "lr": "4.98749e-05", "gnorm": "3.145", "train_wall": "38", "wall": "1499"}
2021-10-24 23:09:01 | INFO | train_inner | {"epoch": 3, "update": 2.531, "loss": "2.588", "nll_loss": "0.679", "ppl": "1.6", "wps": "779.4", "ups": "2.75", "wpb": "283.2", "bsz": "16", "num_updates": "3100", "lr": "4.98699e-05", "gnorm": "2.883", "train_wall": "36", "wall": "1536"}
2021-10-24 23:09:38 | INFO | train_inner | {"epoch": 3, "update": 2.612, "loss": "2.619", "nll_loss": "0.712", "ppl": "1.64", "wps": "753.1", "ups": "2.73", "wpb": "275.4", "bsz": "16", "num_updates": "3200", "lr": "4.98649e-05", "gnorm": "3.04", "train_wall": "36", "wall": "1572"}
2021-10-24 23:10:14 | INFO | train_inner | {"epoch": 3, "update": 2.694, "loss": "2.565", "nll_loss": "0.653", "ppl": "1.57", "wps": "816.5", "ups": "2.75", "wpb": "296.6", "bsz": "16", "num_updates": "3300", "lr": "4.98599e-05", "gnorm": "3.042", "train_wall": "36", "wall": "1609"}
2021-10-24 23:10:51 | INFO | train_inner | {"epoch": 3, "update": 2.776, "loss": "2.579", "nll_loss": "0.67", "ppl": "1.59", "wps": "872", "ups": "2.71", "wpb": "322.2", "bsz": "16", "num_updates": "3400", "lr": "4.98549e-05", "gnorm": "2.842", "train_wall": "37", "wall": "1645"}
2021-10-24 23:11:28 | INFO | train_inner | {"epoch": 3, "update": 2.857, "loss": "2.611", "nll_loss": "0.706", "ppl": "1.63", "wps": "731.8", "ups": "2.69", "wpb": "272.5", "bsz": "16", "num_updates": "3500", "lr": "4.98499e-05", "gnorm": "3.074", "train_wall": "37", "wall": "1683"}
2021-10-24 23:12:06 | INFO | train_inner | {"epoch": 3, "update": 2.939, "loss": "2.513", "nll_loss": "0.599", "ppl": "1.52", "wps": "949.8", "ups": "2.68", "wpb": "354.4", "bsz": "16", "num_updates": "3600", "lr": "4.98449e-05", "gnorm": "2.949", "train_wall": "37", "wall": "1720"}
2021-10-24 23:12:32 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 23:12:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 23:15:51 | INFO | valid | {"epoch": 3, "valid_loss": "2.856", "valid_nll_loss": "0.827", "valid_ppl": "1.77", "valid_bleu": "64.91", "valid_wps": "224.9", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "3675", "valid_best_bleu": "66.26"}
2021-10-24 23:15:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 23:15:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 3 @ 3675 updates, score 64.91) (writing took 6.928208647994325 seconds)
2021-10-24 23:15:58 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-10-24 23:15:58 | INFO | train | {"epoch": 3, "train_loss": "2.57", "train_nll_loss": "0.657", "train_ppl": "1.58", "train_wps": "565", "train_ups": "1.85", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "3675", "train_lr": "4.98412e-05", "train_gnorm": "3.027", "train_train_wall": "451", "train_wall": "1952"}
2021-10-24 23:15:58 | INFO | fairseq_cli.train | begin training epoch 3
2021-10-24 23:16:08 | INFO | train_inner | {"epoch": 4, "update": 3.02, "loss": "2.542", "nll_loss": "0.63", "ppl": "1.55", "wps": "124.7", "ups": "0.41", "wpb": "302.1", "bsz": "15.9", "num_updates": "3700", "lr": "4.98399e-05", "gnorm": "3.055", "train_wall": "35", "wall": "1962"}
2021-10-24 23:16:46 | INFO | train_inner | {"epoch": 4, "update": 3.102, "loss": "2.406", "nll_loss": "0.474", "ppl": "1.39", "wps": "889.9", "ups": "2.65", "wpb": "335.3", "bsz": "16", "num_updates": "3800", "lr": "4.98349e-05", "gnorm": "2.687", "train_wall": "37", "wall": "2000"}
2021-10-24 23:17:20 | INFO | train_inner | {"epoch": 4, "update": 3.184, "loss": "2.46", "nll_loss": "0.531", "ppl": "1.44", "wps": "913.7", "ups": "2.93", "wpb": "311.7", "bsz": "16", "num_updates": "3900", "lr": "4.98299e-05", "gnorm": "3.186", "train_wall": "34", "wall": "2034"}
2021-10-24 23:17:54 | INFO | train_inner | {"epoch": 4, "update": 3.265, "loss": "2.467", "nll_loss": "0.542", "ppl": "1.46", "wps": "844.3", "ups": "2.96", "wpb": "285.5", "bsz": "16", "num_updates": "4000", "lr": "4.98249e-05", "gnorm": "3.249", "train_wall": "34", "wall": "2068"}
2021-10-24 23:18:31 | INFO | train_inner | {"epoch": 4, "update": 3.347, "loss": "2.429", "nll_loss": "0.498", "ppl": "1.41", "wps": "902.9", "ups": "2.71", "wpb": "333.2", "bsz": "16", "num_updates": "4100", "lr": "4.98199e-05", "gnorm": "2.751", "train_wall": "37", "wall": "2105"}
2021-10-24 23:19:08 | INFO | train_inner | {"epoch": 4, "update": 3.429, "loss": "2.486", "nll_loss": "0.563", "ppl": "1.48", "wps": "802.2", "ups": "2.64", "wpb": "303.4", "bsz": "16", "num_updates": "4200", "lr": "4.98149e-05", "gnorm": "3.204", "train_wall": "38", "wall": "2143"}
2021-10-24 23:19:46 | INFO | train_inner | {"epoch": 4, "update": 3.51, "loss": "2.437", "nll_loss": "0.512", "ppl": "1.43", "wps": "811.6", "ups": "2.63", "wpb": "308.7", "bsz": "16", "num_updates": "4300", "lr": "4.98099e-05", "gnorm": "2.901", "train_wall": "38", "wall": "2181"}
2021-10-24 23:20:22 | INFO | train_inner | {"epoch": 4, "update": 3.592, "loss": "2.473", "nll_loss": "0.55", "ppl": "1.46", "wps": "833.7", "ups": "2.82", "wpb": "296.1", "bsz": "16", "num_updates": "4400", "lr": "4.98049e-05", "gnorm": "2.958", "train_wall": "35", "wall": "2216"}
2021-10-24 23:21:01 | INFO | train_inner | {"epoch": 4, "update": 3.673, "loss": "2.478", "nll_loss": "0.561", "ppl": "1.47", "wps": "753.4", "ups": "2.57", "wpb": "292.6", "bsz": "16", "num_updates": "4500", "lr": "4.97999e-05", "gnorm": "3.05", "train_wall": "39", "wall": "2255"}
2021-10-24 23:21:39 | INFO | train_inner | {"epoch": 4, "update": 3.755, "loss": "2.481", "nll_loss": "0.563", "ppl": "1.48", "wps": "743.1", "ups": "2.6", "wpb": "286.1", "bsz": "16", "num_updates": "4600", "lr": "4.97949e-05", "gnorm": "3.053", "train_wall": "38", "wall": "2294"}
2021-10-24 23:22:17 | INFO | train_inner | {"epoch": 4, "update": 3.837, "loss": "2.481", "nll_loss": "0.563", "ppl": "1.48", "wps": "863.3", "ups": "2.68", "wpb": "321.9", "bsz": "16", "num_updates": "4700", "lr": "4.97899e-05", "gnorm": "2.877", "train_wall": "37", "wall": "2331"}
2021-10-24 23:22:51 | INFO | train_inner | {"epoch": 4, "update": 3.918, "loss": "2.525", "nll_loss": "0.612", "ppl": "1.53", "wps": "747.1", "ups": "2.87", "wpb": "260.5", "bsz": "16", "num_updates": "4800", "lr": "4.97849e-05", "gnorm": "3.201", "train_wall": "35", "wall": "2366"}
2021-10-24 23:23:27 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 23:23:27 | INFO | train_inner | {"epoch": 4, "update": 4.0, "loss": "2.478", "nll_loss": "0.559", "ppl": "1.47", "wps": "863.7", "ups": "2.79", "wpb": "310.1", "bsz": "15.9", "num_updates": "4900", "lr": "4.97799e-05", "gnorm": "3.008", "train_wall": "36", "wall": "2402"}
2021-10-24 23:23:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 23:26:46 | INFO | valid | {"epoch": 4, "valid_loss": "2.876", "valid_nll_loss": "0.841", "valid_ppl": "1.79", "valid_bleu": "67.06", "valid_wps": "224.9", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "4900", "valid_best_bleu": "67.06"}
2021-10-24 23:26:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 23:27:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_best.pt (epoch 4 @ 4900 updates, score 67.06) (writing took 25.056776177021675 seconds)
2021-10-24 23:27:11 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-10-24 23:27:12 | INFO | train | {"epoch": 4, "train_loss": "2.463", "train_nll_loss": "0.54", "train_ppl": "1.45", "train_wps": "554.4", "train_ups": "1.82", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "4900", "train_lr": "4.97799e-05", "train_gnorm": "3", "train_train_wall": "445", "train_wall": "2626"}
2021-10-24 23:27:12 | INFO | fairseq_cli.train | begin training epoch 4
2021-10-24 23:27:49 | INFO | train_inner | {"epoch": 5, "update": 4.082, "loss": "2.335", "nll_loss": "0.399", "ppl": "1.32", "wps": "121.3", "ups": "0.38", "wpb": "317.9", "bsz": "16", "num_updates": "5000", "lr": "4.97749e-05", "gnorm": "2.534", "train_wall": "36", "wall": "2664"}
2021-10-24 23:28:27 | INFO | train_inner | {"epoch": 5, "update": 4.163, "loss": "2.367", "nll_loss": "0.432", "ppl": "1.35", "wps": "749.4", "ups": "2.69", "wpb": "278.2", "bsz": "16", "num_updates": "5100", "lr": "4.97699e-05", "gnorm": "2.691", "train_wall": "37", "wall": "2701"}
2021-10-24 23:29:04 | INFO | train_inner | {"epoch": 5, "update": 4.245, "loss": "2.393", "nll_loss": "0.458", "ppl": "1.37", "wps": "709.6", "ups": "2.64", "wpb": "268.8", "bsz": "16", "num_updates": "5200", "lr": "4.97649e-05", "gnorm": "2.971", "train_wall": "38", "wall": "2739"}
2021-10-24 23:29:43 | INFO | train_inner | {"epoch": 5, "update": 4.327, "loss": "2.373", "nll_loss": "0.442", "ppl": "1.36", "wps": "747.4", "ups": "2.61", "wpb": "286", "bsz": "16", "num_updates": "5300", "lr": "4.97599e-05", "gnorm": "2.941", "train_wall": "38", "wall": "2777"}
2021-10-24 23:30:18 | INFO | train_inner | {"epoch": 5, "update": 4.408, "loss": "2.37", "nll_loss": "0.435", "ppl": "1.35", "wps": "871.9", "ups": "2.82", "wpb": "309.1", "bsz": "16", "num_updates": "5400", "lr": "4.97549e-05", "gnorm": "2.782", "train_wall": "35", "wall": "2812"}
2021-10-24 23:30:55 | INFO | train_inner | {"epoch": 5, "update": 4.49, "loss": "2.374", "nll_loss": "0.44", "ppl": "1.36", "wps": "918.3", "ups": "2.73", "wpb": "336", "bsz": "16", "num_updates": "5500", "lr": "4.97499e-05", "gnorm": "3.085", "train_wall": "36", "wall": "2849"}
2021-10-24 23:31:29 | INFO | train_inner | {"epoch": 5, "update": 4.571, "loss": "2.365", "nll_loss": "0.437", "ppl": "1.35", "wps": "851.8", "ups": "2.93", "wpb": "290.5", "bsz": "16", "num_updates": "5600", "lr": "4.97449e-05", "gnorm": "2.594", "train_wall": "34", "wall": "2883"}
2021-10-24 23:32:06 | INFO | train_inner | {"epoch": 5, "update": 4.653, "loss": "2.383", "nll_loss": "0.454", "ppl": "1.37", "wps": "878.8", "ups": "2.71", "wpb": "324.5", "bsz": "16", "num_updates": "5700", "lr": "4.97399e-05", "gnorm": "2.811", "train_wall": "37", "wall": "2920"}
2021-10-24 23:32:40 | INFO | train_inner | {"epoch": 5, "update": 4.735, "loss": "2.385", "nll_loss": "0.456", "ppl": "1.37", "wps": "858.4", "ups": "2.92", "wpb": "294.1", "bsz": "16", "num_updates": "5800", "lr": "4.97349e-05", "gnorm": "2.907", "train_wall": "34", "wall": "2954"}
2021-10-24 23:33:16 | INFO | train_inner | {"epoch": 5, "update": 4.816, "loss": "2.4", "nll_loss": "0.473", "ppl": "1.39", "wps": "880.3", "ups": "2.78", "wpb": "316.4", "bsz": "16", "num_updates": "5900", "lr": "4.97299e-05", "gnorm": "3.08", "train_wall": "36", "wall": "2990"}
2021-10-24 23:33:55 | INFO | train_inner | {"epoch": 5, "update": 4.898, "loss": "2.374", "nll_loss": "0.447", "ppl": "1.36", "wps": "865", "ups": "2.56", "wpb": "337.2", "bsz": "16", "num_updates": "6000", "lr": "4.97249e-05", "gnorm": "2.892", "train_wall": "39", "wall": "3029"}
2021-10-24 23:34:31 | INFO | train_inner | {"epoch": 5, "update": 4.98, "loss": "2.437", "nll_loss": "0.515", "ppl": "1.43", "wps": "771.5", "ups": "2.76", "wpb": "279.3", "bsz": "16", "num_updates": "6100", "lr": "4.97199e-05", "gnorm": "3.331", "train_wall": "36", "wall": "3065"}
2021-10-24 23:34:40 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 23:34:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 23:37:59 | INFO | valid | {"epoch": 5, "valid_loss": "2.891", "valid_nll_loss": "0.883", "valid_ppl": "1.84", "valid_bleu": "67.26", "valid_wps": "225.7", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "6125", "valid_best_bleu": "67.26"}
2021-10-24 23:37:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 23:38:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_best.pt (epoch 5 @ 6125 updates, score 67.26) (writing took 23.57450993009843 seconds)
2021-10-24 23:38:22 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-10-24 23:38:22 | INFO | train | {"epoch": 5, "train_loss": "2.378", "train_nll_loss": "0.447", "train_ppl": "1.36", "train_wps": "557.3", "train_ups": "1.83", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "6125", "train_lr": "4.97186e-05", "train_gnorm": "2.893", "train_train_wall": "444", "train_wall": "3296"}
2021-10-24 23:38:22 | INFO | fairseq_cli.train | begin training epoch 5
2021-10-24 23:38:49 | INFO | train_inner | {"epoch": 6, "update": 5.061, "loss": "2.308", "nll_loss": "0.37", "ppl": "1.29", "wps": "141.1", "ups": "0.39", "wpb": "363.7", "bsz": "15.9", "num_updates": "6200", "lr": "4.97149e-05", "gnorm": "2.689", "train_wall": "34", "wall": "3323"}
2021-10-24 23:39:20 | INFO | train_inner | {"epoch": 6, "update": 5.143, "loss": "2.277", "nll_loss": "0.336", "ppl": "1.26", "wps": "1029", "ups": "3.18", "wpb": "323.4", "bsz": "16", "num_updates": "6300", "lr": "4.97099e-05", "gnorm": "2.463", "train_wall": "31", "wall": "3355"}
2021-10-24 23:39:54 | INFO | train_inner | {"epoch": 6, "update": 5.224, "loss": "2.298", "nll_loss": "0.359", "ppl": "1.28", "wps": "874.8", "ups": "3.01", "wpb": "290.7", "bsz": "16", "num_updates": "6400", "lr": "4.97049e-05", "gnorm": "2.613", "train_wall": "33", "wall": "3388"}
2021-10-24 23:40:31 | INFO | train_inner | {"epoch": 6, "update": 5.306, "loss": "2.296", "nll_loss": "0.357", "ppl": "1.28", "wps": "891", "ups": "2.71", "wpb": "328.7", "bsz": "16", "num_updates": "6500", "lr": "4.96998e-05", "gnorm": "2.632", "train_wall": "37", "wall": "3425"}
2021-10-24 23:41:06 | INFO | train_inner | {"epoch": 6, "update": 5.388, "loss": "2.301", "nll_loss": "0.361", "ppl": "1.28", "wps": "933.1", "ups": "2.84", "wpb": "329.1", "bsz": "16", "num_updates": "6600", "lr": "4.96948e-05", "gnorm": "2.722", "train_wall": "35", "wall": "3460"}
2021-10-24 23:41:41 | INFO | train_inner | {"epoch": 6, "update": 5.469, "loss": "2.308", "nll_loss": "0.374", "ppl": "1.3", "wps": "819.6", "ups": "2.85", "wpb": "287.1", "bsz": "16", "num_updates": "6700", "lr": "4.96898e-05", "gnorm": "2.635", "train_wall": "35", "wall": "3495"}
2021-10-24 23:42:15 | INFO | train_inner | {"epoch": 6, "update": 5.551, "loss": "2.308", "nll_loss": "0.372", "ppl": "1.29", "wps": "880.4", "ups": "2.93", "wpb": "300.9", "bsz": "16", "num_updates": "6800", "lr": "4.96848e-05", "gnorm": "2.622", "train_wall": "34", "wall": "3529"}
2021-10-24 23:42:51 | INFO | train_inner | {"epoch": 6, "update": 5.633, "loss": "2.324", "nll_loss": "0.388", "ppl": "1.31", "wps": "796.5", "ups": "2.78", "wpb": "286.4", "bsz": "16", "num_updates": "6900", "lr": "4.96798e-05", "gnorm": "2.839", "train_wall": "36", "wall": "3565"}
2021-10-24 23:43:25 | INFO | train_inner | {"epoch": 6, "update": 5.714, "loss": "2.308", "nll_loss": "0.374", "ppl": "1.3", "wps": "1025", "ups": "2.9", "wpb": "352.9", "bsz": "16", "num_updates": "7000", "lr": "4.96748e-05", "gnorm": "2.712", "train_wall": "34", "wall": "3600"}
2021-10-24 23:44:01 | INFO | train_inner | {"epoch": 6, "update": 5.796, "loss": "2.333", "nll_loss": "0.403", "ppl": "1.32", "wps": "772.6", "ups": "2.79", "wpb": "276.4", "bsz": "16", "num_updates": "7100", "lr": "4.96698e-05", "gnorm": "2.706", "train_wall": "36", "wall": "3635"}
2021-10-24 23:44:34 | INFO | train_inner | {"epoch": 6, "update": 5.878, "loss": "2.351", "nll_loss": "0.421", "ppl": "1.34", "wps": "914.3", "ups": "3.07", "wpb": "297.8", "bsz": "16", "num_updates": "7200", "lr": "4.96648e-05", "gnorm": "3.073", "train_wall": "32", "wall": "3668"}
2021-10-24 23:45:06 | INFO | train_inner | {"epoch": 6, "update": 5.959, "loss": "2.38", "nll_loss": "0.451", "ppl": "1.37", "wps": "849.3", "ups": "3.07", "wpb": "277", "bsz": "16", "num_updates": "7300", "lr": "4.96598e-05", "gnorm": "3.154", "train_wall": "32", "wall": "3701"}
2021-10-24 23:45:22 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 23:45:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 23:48:49 | INFO | valid | {"epoch": 6, "valid_loss": "2.944", "valid_nll_loss": "0.909", "valid_ppl": "1.88", "valid_bleu": "65.96", "valid_wps": "216.1", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "7350", "valid_best_bleu": "67.26"}
2021-10-24 23:48:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 23:49:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 6 @ 7350 updates, score 65.96) (writing took 16.198250381974503 seconds)
2021-10-24 23:49:06 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-10-24 23:49:06 | INFO | train | {"epoch": 6, "train_loss": "2.316", "train_nll_loss": "0.381", "train_ppl": "1.3", "train_wps": "580.7", "train_ups": "1.9", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "7350", "train_lr": "4.96573e-05", "train_gnorm": "2.751", "train_train_wall": "416", "train_wall": "3940"}
2021-10-24 23:49:06 | INFO | fairseq_cli.train | begin training epoch 6
2021-10-24 23:49:26 | INFO | train_inner | {"epoch": 7, "update": 6.041, "loss": "2.317", "nll_loss": "0.381", "ppl": "1.3", "wps": "104.8", "ups": "0.39", "wpb": "271.6", "bsz": "15.9", "num_updates": "7400", "lr": "4.96548e-05", "gnorm": "2.881", "train_wall": "34", "wall": "3960"}
2021-10-24 23:50:03 | INFO | train_inner | {"epoch": 7, "update": 6.122, "loss": "2.266", "nll_loss": "0.325", "ppl": "1.25", "wps": "854.4", "ups": "2.67", "wpb": "319.4", "bsz": "16", "num_updates": "7500", "lr": "4.96498e-05", "gnorm": "2.526", "train_wall": "37", "wall": "3997"}
2021-10-24 23:50:37 | INFO | train_inner | {"epoch": 7, "update": 6.204, "loss": "2.27", "nll_loss": "0.329", "ppl": "1.26", "wps": "835.3", "ups": "2.95", "wpb": "282.7", "bsz": "16", "num_updates": "7600", "lr": "4.96448e-05", "gnorm": "2.515", "train_wall": "34", "wall": "4031"}
2021-10-24 23:51:14 | INFO | train_inner | {"epoch": 7, "update": 6.286, "loss": "2.257", "nll_loss": "0.315", "ppl": "1.24", "wps": "871.1", "ups": "2.71", "wpb": "321.4", "bsz": "16", "num_updates": "7700", "lr": "4.96398e-05", "gnorm": "2.597", "train_wall": "37", "wall": "4068"}
2021-10-24 23:51:51 | INFO | train_inner | {"epoch": 7, "update": 6.367, "loss": "2.283", "nll_loss": "0.344", "ppl": "1.27", "wps": "850.3", "ups": "2.72", "wpb": "312.5", "bsz": "16", "num_updates": "7800", "lr": "4.96348e-05", "gnorm": "2.766", "train_wall": "37", "wall": "4105"}
2021-10-24 23:52:24 | INFO | train_inner | {"epoch": 7, "update": 6.449, "loss": "2.29", "nll_loss": "0.356", "ppl": "1.28", "wps": "771.7", "ups": "2.95", "wpb": "261.7", "bsz": "16", "num_updates": "7900", "lr": "4.96298e-05", "gnorm": "2.663", "train_wall": "34", "wall": "4139"}
2021-10-24 23:52:58 | INFO | train_inner | {"epoch": 7, "update": 6.531, "loss": "2.285", "nll_loss": "0.348", "ppl": "1.27", "wps": "860.3", "ups": "2.97", "wpb": "289.6", "bsz": "16", "num_updates": "8000", "lr": "4.96248e-05", "gnorm": "2.764", "train_wall": "33", "wall": "4172"}
2021-10-24 23:53:31 | INFO | train_inner | {"epoch": 7, "update": 6.612, "loss": "2.277", "nll_loss": "0.342", "ppl": "1.27", "wps": "875.3", "ups": "3.05", "wpb": "286.7", "bsz": "16", "num_updates": "8100", "lr": "4.96198e-05", "gnorm": "2.719", "train_wall": "33", "wall": "4205"}
2021-10-24 23:54:06 | INFO | train_inner | {"epoch": 7, "update": 6.694, "loss": "2.278", "nll_loss": "0.343", "ppl": "1.27", "wps": "931.3", "ups": "2.82", "wpb": "330.4", "bsz": "16", "num_updates": "8200", "lr": "4.96148e-05", "gnorm": "2.613", "train_wall": "35", "wall": "4241"}
2021-10-24 23:54:37 | INFO | train_inner | {"epoch": 7, "update": 6.776, "loss": "2.289", "nll_loss": "0.351", "ppl": "1.28", "wps": "1007.2", "ups": "3.25", "wpb": "310.2", "bsz": "16", "num_updates": "8300", "lr": "4.96098e-05", "gnorm": "2.685", "train_wall": "31", "wall": "4271"}
2021-10-24 23:55:09 | INFO | train_inner | {"epoch": 7, "update": 6.857, "loss": "2.29", "nll_loss": "0.356", "ppl": "1.28", "wps": "984.9", "ups": "3.18", "wpb": "309.7", "bsz": "16", "num_updates": "8400", "lr": "4.96048e-05", "gnorm": "2.837", "train_wall": "31", "wall": "4303"}
2021-10-24 23:55:40 | INFO | train_inner | {"epoch": 7, "update": 6.939, "loss": "2.271", "nll_loss": "0.339", "ppl": "1.27", "wps": "1065.6", "ups": "3.16", "wpb": "337.5", "bsz": "16", "num_updates": "8500", "lr": "4.95998e-05", "gnorm": "2.815", "train_wall": "31", "wall": "4334"}
2021-10-24 23:56:08 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-24 23:56:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-24 23:59:22 | INFO | valid | {"epoch": 7, "valid_loss": "2.938", "valid_nll_loss": "0.941", "valid_ppl": "1.92", "valid_bleu": "65.86", "valid_wps": "230.2", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "8575", "valid_best_bleu": "67.26"}
2021-10-24 23:59:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-24 23:59:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 7 @ 8575 updates, score 65.86) (writing took 15.635370184085332 seconds)
2021-10-24 23:59:38 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-10-24 23:59:38 | INFO | train | {"epoch": 7, "train_loss": "2.277", "train_nll_loss": "0.34", "train_ppl": "1.27", "train_wps": "591", "train_ups": "1.94", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "8575", "train_lr": "4.9596e-05", "train_gnorm": "2.672", "train_train_wall": "418", "train_wall": "4572"}
2021-10-24 23:59:38 | INFO | fairseq_cli.train | begin training epoch 7
2021-10-24 23:59:48 | INFO | train_inner | {"epoch": 8, "update": 7.02, "loss": "2.276", "nll_loss": "0.343", "ppl": "1.27", "wps": "119.6", "ups": "0.4", "wpb": "296.7", "bsz": "15.9", "num_updates": "8600", "lr": "4.95948e-05", "gnorm": "2.571", "train_wall": "36", "wall": "4583"}
2021-10-25 00:00:23 | INFO | train_inner | {"epoch": 8, "update": 7.102, "loss": "2.244", "nll_loss": "0.305", "ppl": "1.24", "wps": "770.2", "ups": "2.87", "wpb": "268.8", "bsz": "16", "num_updates": "8700", "lr": "4.95898e-05", "gnorm": "2.677", "train_wall": "35", "wall": "4617"}
2021-10-25 00:00:59 | INFO | train_inner | {"epoch": 8, "update": 7.184, "loss": "2.234", "nll_loss": "0.294", "ppl": "1.23", "wps": "880.8", "ups": "2.76", "wpb": "318.9", "bsz": "16", "num_updates": "8800", "lr": "4.95848e-05", "gnorm": "2.523", "train_wall": "36", "wall": "4654"}
2021-10-25 00:01:32 | INFO | train_inner | {"epoch": 8, "update": 7.265, "loss": "2.245", "nll_loss": "0.305", "ppl": "1.24", "wps": "956", "ups": "3.04", "wpb": "314.6", "bsz": "16", "num_updates": "8900", "lr": "4.95798e-05", "gnorm": "2.361", "train_wall": "33", "wall": "4687"}
2021-10-25 00:02:09 | INFO | train_inner | {"epoch": 8, "update": 7.347, "loss": "2.238", "nll_loss": "0.303", "ppl": "1.23", "wps": "884.2", "ups": "2.7", "wpb": "327.9", "bsz": "16", "num_updates": "9000", "lr": "4.95748e-05", "gnorm": "2.45", "train_wall": "37", "wall": "4724"}
2021-10-25 00:02:45 | INFO | train_inner | {"epoch": 8, "update": 7.429, "loss": "2.244", "nll_loss": "0.309", "ppl": "1.24", "wps": "816.5", "ups": "2.8", "wpb": "291.8", "bsz": "16", "num_updates": "9100", "lr": "4.95698e-05", "gnorm": "2.653", "train_wall": "35", "wall": "4759"}
2021-10-25 00:03:18 | INFO | train_inner | {"epoch": 8, "update": 7.51, "loss": "2.244", "nll_loss": "0.305", "ppl": "1.24", "wps": "1003.8", "ups": "3.07", "wpb": "327.5", "bsz": "16", "num_updates": "9200", "lr": "4.95648e-05", "gnorm": "2.675", "train_wall": "32", "wall": "4792"}
2021-10-25 00:03:56 | INFO | train_inner | {"epoch": 8, "update": 7.592, "loss": "2.253", "nll_loss": "0.319", "ppl": "1.25", "wps": "740.7", "ups": "2.65", "wpb": "279.7", "bsz": "16", "num_updates": "9300", "lr": "4.95598e-05", "gnorm": "2.638", "train_wall": "38", "wall": "4830"}
2021-10-25 00:04:32 | INFO | train_inner | {"epoch": 8, "update": 7.673, "loss": "2.267", "nll_loss": "0.331", "ppl": "1.26", "wps": "738.5", "ups": "2.73", "wpb": "270.9", "bsz": "16", "num_updates": "9400", "lr": "4.95548e-05", "gnorm": "2.995", "train_wall": "36", "wall": "4866"}
2021-10-25 00:05:06 | INFO | train_inner | {"epoch": 8, "update": 7.755, "loss": "2.263", "nll_loss": "0.324", "ppl": "1.25", "wps": "840.5", "ups": "2.93", "wpb": "286.7", "bsz": "16", "num_updates": "9500", "lr": "4.95498e-05", "gnorm": "2.911", "train_wall": "34", "wall": "4901"}
2021-10-25 00:05:42 | INFO | train_inner | {"epoch": 8, "update": 7.837, "loss": "2.258", "nll_loss": "0.323", "ppl": "1.25", "wps": "854.2", "ups": "2.82", "wpb": "302.9", "bsz": "16", "num_updates": "9600", "lr": "4.95448e-05", "gnorm": "2.869", "train_wall": "35", "wall": "4936"}
2021-10-25 00:06:18 | INFO | train_inner | {"epoch": 8, "update": 7.918, "loss": "2.246", "nll_loss": "0.317", "ppl": "1.25", "wps": "1070.9", "ups": "2.8", "wpb": "383.1", "bsz": "16", "num_updates": "9700", "lr": "4.95398e-05", "gnorm": "2.629", "train_wall": "36", "wall": "4972"}
2021-10-25 00:06:53 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-25 00:06:53 | INFO | train_inner | {"epoch": 8, "update": 8.0, "loss": "2.26", "nll_loss": "0.328", "ppl": "1.26", "wps": "813.9", "ups": "2.78", "wpb": "292.5", "bsz": "15.9", "num_updates": "9800", "lr": "4.95348e-05", "gnorm": "2.818", "train_wall": "36", "wall": "5008"}
2021-10-25 00:06:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-25 00:10:11 | INFO | valid | {"epoch": 8, "valid_loss": "2.986", "valid_nll_loss": "0.985", "valid_ppl": "1.98", "valid_bleu": "64.32", "valid_wps": "227", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "9800", "valid_best_bleu": "67.26"}
2021-10-25 00:10:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-25 00:10:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 8 @ 9800 updates, score 64.32) (writing took 7.5643352180486545 seconds)
2021-10-25 00:10:18 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-10-25 00:10:18 | INFO | train | {"epoch": 8, "train_loss": "2.249", "train_nll_loss": "0.313", "train_ppl": "1.24", "train_wps": "583.7", "train_ups": "1.91", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "9800", "train_lr": "4.95348e-05", "train_gnorm": "2.674", "train_train_wall": "431", "train_wall": "5212"}
2021-10-25 00:10:18 | INFO | fairseq_cli.train | begin training epoch 8
2021-10-25 00:10:55 | INFO | train_inner | {"epoch": 9, "update": 8.082, "loss": "2.215", "nll_loss": "0.277", "ppl": "1.21", "wps": "130.7", "ups": "0.41", "wpb": "315.4", "bsz": "16", "num_updates": "9900", "lr": "4.95298e-05", "gnorm": "2.442", "train_wall": "35", "wall": "5249"}
2021-10-25 00:11:30 | INFO | train_inner | {"epoch": 9, "update": 8.163, "loss": "2.209", "nll_loss": "0.269", "ppl": "1.2", "wps": "836.3", "ups": "2.83", "wpb": "295.2", "bsz": "16", "num_updates": "10000", "lr": "4.95248e-05", "gnorm": "2.476", "train_wall": "35", "wall": "5284"}
2021-10-25 00:12:04 | INFO | train_inner | {"epoch": 9, "update": 8.245, "loss": "2.215", "nll_loss": "0.278", "ppl": "1.21", "wps": "896.7", "ups": "2.91", "wpb": "307.7", "bsz": "16", "num_updates": "10100", "lr": "4.95198e-05", "gnorm": "2.237", "train_wall": "34", "wall": "5319"}
2021-10-25 00:12:36 | INFO | train_inner | {"epoch": 9, "update": 8.327, "loss": "2.216", "nll_loss": "0.284", "ppl": "1.22", "wps": "951.3", "ups": "3.19", "wpb": "298.5", "bsz": "16", "num_updates": "10200", "lr": "4.95148e-05", "gnorm": "2.384", "train_wall": "31", "wall": "5350"}
2021-10-25 00:13:08 | INFO | train_inner | {"epoch": 9, "update": 8.408, "loss": "2.225", "nll_loss": "0.29", "ppl": "1.22", "wps": "954", "ups": "3.07", "wpb": "310.8", "bsz": "16", "num_updates": "10300", "lr": "4.95098e-05", "gnorm": "2.673", "train_wall": "32", "wall": "5383"}
2021-10-25 00:13:43 | INFO | train_inner | {"epoch": 9, "update": 8.49, "loss": "2.215", "nll_loss": "0.279", "ppl": "1.21", "wps": "907", "ups": "2.9", "wpb": "313", "bsz": "16", "num_updates": "10400", "lr": "4.95048e-05", "gnorm": "2.435", "train_wall": "34", "wall": "5417"}
2021-10-25 00:14:14 | INFO | train_inner | {"epoch": 9, "update": 8.571, "loss": "2.233", "nll_loss": "0.297", "ppl": "1.23", "wps": "1034.4", "ups": "3.24", "wpb": "318.9", "bsz": "16", "num_updates": "10500", "lr": "4.94997e-05", "gnorm": "2.587", "train_wall": "31", "wall": "5448"}
2021-10-25 00:14:48 | INFO | train_inner | {"epoch": 9, "update": 8.653, "loss": "2.243", "nll_loss": "0.309", "ppl": "1.24", "wps": "873.5", "ups": "2.9", "wpb": "301.2", "bsz": "16", "num_updates": "10600", "lr": "4.94947e-05", "gnorm": "2.796", "train_wall": "34", "wall": "5482"}
2021-10-25 00:15:24 | INFO | train_inner | {"epoch": 9, "update": 8.735, "loss": "2.235", "nll_loss": "0.303", "ppl": "1.23", "wps": "810.1", "ups": "2.8", "wpb": "289.6", "bsz": "16", "num_updates": "10700", "lr": "4.94897e-05", "gnorm": "2.486", "train_wall": "36", "wall": "5518"}
2021-10-25 00:15:57 | INFO | train_inner | {"epoch": 9, "update": 8.816, "loss": "2.247", "nll_loss": "0.312", "ppl": "1.24", "wps": "819.4", "ups": "2.98", "wpb": "274.6", "bsz": "16", "num_updates": "10800", "lr": "4.94847e-05", "gnorm": "2.784", "train_wall": "33", "wall": "5552"}
2021-10-25 00:16:31 | INFO | train_inner | {"epoch": 9, "update": 8.898, "loss": "2.262", "nll_loss": "0.331", "ppl": "1.26", "wps": "817", "ups": "2.95", "wpb": "277", "bsz": "16", "num_updates": "10900", "lr": "4.94797e-05", "gnorm": "2.908", "train_wall": "34", "wall": "5586"}
2021-10-25 00:17:09 | INFO | train_inner | {"epoch": 9, "update": 8.98, "loss": "2.235", "nll_loss": "0.305", "ppl": "1.24", "wps": "950.8", "ups": "2.66", "wpb": "357.1", "bsz": "16", "num_updates": "11000", "lr": "4.94747e-05", "gnorm": "2.38", "train_wall": "37", "wall": "5623"}
2021-10-25 00:17:18 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-25 00:17:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-25 00:20:32 | INFO | valid | {"epoch": 9, "valid_loss": "3.008", "valid_nll_loss": "1.019", "valid_ppl": "2.03", "valid_bleu": "64.59", "valid_wps": "230.8", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "11025", "valid_best_bleu": "67.26"}
2021-10-25 00:20:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-25 00:20:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 9 @ 11025 updates, score 64.59) (writing took 22.341549774981104 seconds)
2021-10-25 00:20:54 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-10-25 00:20:54 | INFO | train | {"epoch": 9, "train_loss": "2.229", "train_nll_loss": "0.294", "train_ppl": "1.23", "train_wps": "587.4", "train_ups": "1.93", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "11025", "train_lr": "4.94735e-05", "train_gnorm": "2.551", "train_train_wall": "415", "train_wall": "5849"}
2021-10-25 00:20:54 | INFO | fairseq_cli.train | begin training epoch 9
2021-10-25 00:21:22 | INFO | train_inner | {"epoch": 10, "update": 9.061, "loss": "2.203", "nll_loss": "0.267", "ppl": "1.2", "wps": "114.3", "ups": "0.39", "wpb": "289.7", "bsz": "15.9", "num_updates": "11100", "lr": "4.94697e-05", "gnorm": "2.303", "train_wall": "35", "wall": "5877"}
2021-10-25 00:21:58 | INFO | train_inner | {"epoch": 10, "update": 9.143, "loss": "2.21", "nll_loss": "0.273", "ppl": "1.21", "wps": "825.6", "ups": "2.84", "wpb": "291", "bsz": "16", "num_updates": "11200", "lr": "4.94647e-05", "gnorm": "2.27", "train_wall": "35", "wall": "5912"}
2021-10-25 00:22:33 | INFO | train_inner | {"epoch": 10, "update": 9.224, "loss": "2.209", "nll_loss": "0.269", "ppl": "1.2", "wps": "863", "ups": "2.84", "wpb": "303.6", "bsz": "16", "num_updates": "11300", "lr": "4.94597e-05", "gnorm": "2.338", "train_wall": "35", "wall": "5947"}
2021-10-25 00:23:07 | INFO | train_inner | {"epoch": 10, "update": 9.306, "loss": "2.202", "nll_loss": "0.271", "ppl": "1.21", "wps": "884.6", "ups": "2.93", "wpb": "301.6", "bsz": "16", "num_updates": "11400", "lr": "4.94547e-05", "gnorm": "2.452", "train_wall": "34", "wall": "5981"}
2021-10-25 00:23:41 | INFO | train_inner | {"epoch": 10, "update": 9.388, "loss": "2.206", "nll_loss": "0.272", "ppl": "1.21", "wps": "827.3", "ups": "2.91", "wpb": "283.9", "bsz": "16", "num_updates": "11500", "lr": "4.94497e-05", "gnorm": "2.524", "train_wall": "34", "wall": "6016"}
2021-10-25 00:24:20 | INFO | train_inner | {"epoch": 10, "update": 9.469, "loss": "2.195", "nll_loss": "0.261", "ppl": "1.2", "wps": "872.6", "ups": "2.6", "wpb": "336.2", "bsz": "16", "num_updates": "11600", "lr": "4.94447e-05", "gnorm": "2.171", "train_wall": "38", "wall": "6054"}
2021-10-25 00:24:56 | INFO | train_inner | {"epoch": 10, "update": 9.551, "loss": "2.214", "nll_loss": "0.282", "ppl": "1.22", "wps": "869.6", "ups": "2.78", "wpb": "313", "bsz": "16", "num_updates": "11700", "lr": "4.94397e-05", "gnorm": "2.542", "train_wall": "36", "wall": "6090"}
2021-10-25 00:25:30 | INFO | train_inner | {"epoch": 10, "update": 9.633, "loss": "2.209", "nll_loss": "0.277", "ppl": "1.21", "wps": "952.4", "ups": "2.93", "wpb": "325.6", "bsz": "16", "num_updates": "11800", "lr": "4.94347e-05", "gnorm": "2.369", "train_wall": "34", "wall": "6124"}
2021-10-25 00:26:08 | INFO | train_inner | {"epoch": 10, "update": 9.714, "loss": "2.232", "nll_loss": "0.304", "ppl": "1.23", "wps": "826.1", "ups": "2.62", "wpb": "315.4", "bsz": "16", "num_updates": "11900", "lr": "4.94297e-05", "gnorm": "3.037", "train_wall": "38", "wall": "6162"}
2021-10-25 00:26:44 | INFO | train_inner | {"epoch": 10, "update": 9.796, "loss": "2.235", "nll_loss": "0.306", "ppl": "1.24", "wps": "702", "ups": "2.76", "wpb": "254.3", "bsz": "16", "num_updates": "12000", "lr": "4.94247e-05", "gnorm": "2.548", "train_wall": "36", "wall": "6199"}
2021-10-25 00:27:20 | INFO | train_inner | {"epoch": 10, "update": 9.878, "loss": "2.236", "nll_loss": "0.307", "ppl": "1.24", "wps": "908.2", "ups": "2.79", "wpb": "325.4", "bsz": "16", "num_updates": "12100", "lr": "4.94197e-05", "gnorm": "2.746", "train_wall": "36", "wall": "6234"}
2021-10-25 00:27:54 | INFO | train_inner | {"epoch": 10, "update": 9.959, "loss": "2.229", "nll_loss": "0.303", "ppl": "1.23", "wps": "911.7", "ups": "2.95", "wpb": "309.3", "bsz": "16", "num_updates": "12200", "lr": "4.94147e-05", "gnorm": "2.748", "train_wall": "34", "wall": "6268"}
2021-10-25 00:28:11 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-25 00:28:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-25 00:31:33 | INFO | valid | {"epoch": 10, "valid_loss": "3.03", "valid_nll_loss": "1.04", "valid_ppl": "2.06", "valid_bleu": "63.05", "valid_wps": "222.1", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "12250", "valid_best_bleu": "67.26"}
2021-10-25 00:31:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-25 00:31:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 10 @ 12250 updates, score 63.05) (writing took 18.077476267004386 seconds)
2021-10-25 00:31:51 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-10-25 00:31:51 | INFO | train | {"epoch": 10, "train_loss": "2.216", "train_nll_loss": "0.284", "train_ppl": "1.22", "train_wps": "569", "train_ups": "1.87", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "12250", "train_lr": "4.94122e-05", "train_gnorm": "2.512", "train_train_wall": "433", "train_wall": "6505"}
2021-10-25 00:31:51 | INFO | fairseq_cli.train | begin training epoch 10
2021-10-25 00:32:10 | INFO | train_inner | {"epoch": 11, "update": 10.041, "loss": "2.224", "nll_loss": "0.296", "ppl": "1.23", "wps": "117.2", "ups": "0.39", "wpb": "300.2", "bsz": "15.9", "num_updates": "12300", "lr": "4.94097e-05", "gnorm": "2.434", "train_wall": "35", "wall": "6524"}
2021-10-25 00:32:48 | INFO | train_inner | {"epoch": 11, "update": 10.122, "loss": "2.188", "nll_loss": "0.254", "ppl": "1.19", "wps": "810.7", "ups": "2.63", "wpb": "308.1", "bsz": "16", "num_updates": "12400", "lr": "4.94047e-05", "gnorm": "2.179", "train_wall": "38", "wall": "6562"}
2021-10-25 00:33:26 | INFO | train_inner | {"epoch": 11, "update": 10.204, "loss": "2.184", "nll_loss": "0.251", "ppl": "1.19", "wps": "789", "ups": "2.62", "wpb": "300.6", "bsz": "16", "num_updates": "12500", "lr": "4.93997e-05", "gnorm": "2.096", "train_wall": "38", "wall": "6600"}
2021-10-25 00:34:00 | INFO | train_inner | {"epoch": 11, "update": 10.286, "loss": "2.185", "nll_loss": "0.254", "ppl": "1.19", "wps": "902.6", "ups": "2.94", "wpb": "307.3", "bsz": "16", "num_updates": "12600", "lr": "4.93947e-05", "gnorm": "2.093", "train_wall": "34", "wall": "6635"}
2021-10-25 00:34:40 | INFO | train_inner | {"epoch": 11, "update": 10.367, "loss": "2.212", "nll_loss": "0.28", "ppl": "1.21", "wps": "792", "ups": "2.55", "wpb": "311", "bsz": "16", "num_updates": "12700", "lr": "4.93897e-05", "gnorm": "2.613", "train_wall": "39", "wall": "6674"}
2021-10-25 00:35:19 | INFO | train_inner | {"epoch": 11, "update": 10.449, "loss": "2.204", "nll_loss": "0.271", "ppl": "1.21", "wps": "734.8", "ups": "2.56", "wpb": "286.9", "bsz": "16", "num_updates": "12800", "lr": "4.93847e-05", "gnorm": "2.572", "train_wall": "39", "wall": "6713"}
2021-10-25 00:35:52 | INFO | train_inner | {"epoch": 11, "update": 10.531, "loss": "2.207", "nll_loss": "0.277", "ppl": "1.21", "wps": "842.4", "ups": "2.96", "wpb": "284.5", "bsz": "16", "num_updates": "12900", "lr": "4.93797e-05", "gnorm": "2.455", "train_wall": "34", "wall": "6747"}
2021-10-25 00:36:30 | INFO | train_inner | {"epoch": 11, "update": 10.612, "loss": "2.221", "nll_loss": "0.29", "ppl": "1.22", "wps": "892.7", "ups": "2.66", "wpb": "336.2", "bsz": "16", "num_updates": "13000", "lr": "4.93747e-05", "gnorm": "2.607", "train_wall": "37", "wall": "6784"}
2021-10-25 00:37:05 | INFO | train_inner | {"epoch": 11, "update": 10.694, "loss": "2.211", "nll_loss": "0.283", "ppl": "1.22", "wps": "917.4", "ups": "2.9", "wpb": "316.7", "bsz": "16", "num_updates": "13100", "lr": "4.93697e-05", "gnorm": "2.88", "train_wall": "34", "wall": "6819"}
2021-10-25 00:37:39 | INFO | train_inner | {"epoch": 11, "update": 10.776, "loss": "2.214", "nll_loss": "0.287", "ppl": "1.22", "wps": "909.5", "ups": "2.92", "wpb": "311.2", "bsz": "16", "num_updates": "13200", "lr": "4.93647e-05", "gnorm": "2.684", "train_wall": "34", "wall": "6853"}
2021-10-25 00:38:13 | INFO | train_inner | {"epoch": 11, "update": 10.857, "loss": "2.211", "nll_loss": "0.283", "ppl": "1.22", "wps": "824", "ups": "2.9", "wpb": "284.6", "bsz": "16", "num_updates": "13300", "lr": "4.93597e-05", "gnorm": "2.381", "train_wall": "34", "wall": "6888"}
2021-10-25 00:38:47 | INFO | train_inner | {"epoch": 11, "update": 10.939, "loss": "2.214", "nll_loss": "0.29", "ppl": "1.22", "wps": "919.3", "ups": "3", "wpb": "306.9", "bsz": "16", "num_updates": "13400", "lr": "4.93547e-05", "gnorm": "2.556", "train_wall": "33", "wall": "6921"}
2021-10-25 00:39:11 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-25 00:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-25 00:42:36 | INFO | valid | {"epoch": 11, "valid_loss": "3.035", "valid_nll_loss": "1.071", "valid_ppl": "2.1", "valid_bleu": "61.68", "valid_wps": "218.2", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "13475", "valid_best_bleu": "67.26"}
2021-10-25 00:42:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-25 00:42:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 11 @ 13475 updates, score 61.68) (writing took 13.188016683910973 seconds)
2021-10-25 00:42:50 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-10-25 00:42:50 | INFO | train | {"epoch": 11, "train_loss": "2.205", "train_nll_loss": "0.275", "train_ppl": "1.21", "train_wps": "567.5", "train_ups": "1.86", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "13475", "train_lr": "4.93509e-05", "train_gnorm": "2.444", "train_train_wall": "436", "train_wall": "7164"}
2021-10-25 00:42:50 | INFO | fairseq_cli.train | begin training epoch 11
2021-10-25 00:43:01 | INFO | train_inner | {"epoch": 12, "update": 11.02, "loss": "2.204", "nll_loss": "0.276", "ppl": "1.21", "wps": "124.6", "ups": "0.39", "wpb": "316.9", "bsz": "15.9", "num_updates": "13500", "lr": "4.93497e-05", "gnorm": "2.34", "train_wall": "34", "wall": "7175"}
2021-10-25 00:43:36 | INFO | train_inner | {"epoch": 12, "update": 11.102, "loss": "2.176", "nll_loss": "0.246", "ppl": "1.19", "wps": "1004.8", "ups": "2.89", "wpb": "347.5", "bsz": "16", "num_updates": "13600", "lr": "4.93447e-05", "gnorm": "2.112", "train_wall": "34", "wall": "7210"}
2021-10-25 00:44:08 | INFO | train_inner | {"epoch": 12, "update": 11.184, "loss": "2.187", "nll_loss": "0.258", "ppl": "1.2", "wps": "975.4", "ups": "3.11", "wpb": "313.6", "bsz": "16", "num_updates": "13700", "lr": "4.93397e-05", "gnorm": "2.272", "train_wall": "32", "wall": "7242"}
2021-10-25 00:44:44 | INFO | train_inner | {"epoch": 12, "update": 11.265, "loss": "2.192", "nll_loss": "0.26", "ppl": "1.2", "wps": "816.7", "ups": "2.77", "wpb": "295.2", "bsz": "16", "num_updates": "13800", "lr": "4.93347e-05", "gnorm": "2.603", "train_wall": "36", "wall": "7278"}
2021-10-25 00:45:20 | INFO | train_inner | {"epoch": 12, "update": 11.347, "loss": "2.193", "nll_loss": "0.264", "ppl": "1.2", "wps": "772.9", "ups": "2.78", "wpb": "277.7", "bsz": "16", "num_updates": "13900", "lr": "4.93297e-05", "gnorm": "2.379", "train_wall": "36", "wall": "7314"}
2021-10-25 00:45:54 | INFO | train_inner | {"epoch": 12, "update": 11.429, "loss": "2.204", "nll_loss": "0.278", "ppl": "1.21", "wps": "858.6", "ups": "2.94", "wpb": "292.4", "bsz": "16", "num_updates": "14000", "lr": "4.93247e-05", "gnorm": "2.491", "train_wall": "34", "wall": "7348"}
2021-10-25 00:46:27 | INFO | train_inner | {"epoch": 12, "update": 11.51, "loss": "2.201", "nll_loss": "0.276", "ppl": "1.21", "wps": "929.6", "ups": "2.99", "wpb": "311.4", "bsz": "16", "num_updates": "14100", "lr": "4.93197e-05", "gnorm": "2.635", "train_wall": "33", "wall": "7382"}
2021-10-25 00:47:04 | INFO | train_inner | {"epoch": 12, "update": 11.592, "loss": "2.208", "nll_loss": "0.281", "ppl": "1.22", "wps": "816", "ups": "2.75", "wpb": "296.9", "bsz": "16", "num_updates": "14200", "lr": "4.93147e-05", "gnorm": "2.408", "train_wall": "36", "wall": "7418"}
2021-10-25 00:47:39 | INFO | train_inner | {"epoch": 12, "update": 11.673, "loss": "2.192", "nll_loss": "0.265", "ppl": "1.2", "wps": "819.1", "ups": "2.87", "wpb": "285", "bsz": "16", "num_updates": "14300", "lr": "4.93097e-05", "gnorm": "2.178", "train_wall": "35", "wall": "7453"}
2021-10-25 00:48:10 | INFO | train_inner | {"epoch": 12, "update": 11.755, "loss": "2.211", "nll_loss": "0.286", "ppl": "1.22", "wps": "901.3", "ups": "3.16", "wpb": "284.8", "bsz": "16", "num_updates": "14400", "lr": "4.93047e-05", "gnorm": "2.568", "train_wall": "31", "wall": "7484"}
2021-10-25 00:48:47 | INFO | train_inner | {"epoch": 12, "update": 11.837, "loss": "2.214", "nll_loss": "0.287", "ppl": "1.22", "wps": "868.5", "ups": "2.74", "wpb": "316.9", "bsz": "16", "num_updates": "14500", "lr": "4.92996e-05", "gnorm": "2.79", "train_wall": "36", "wall": "7521"}
2021-10-25 00:49:23 | INFO | train_inner | {"epoch": 12, "update": 11.918, "loss": "2.204", "nll_loss": "0.278", "ppl": "1.21", "wps": "881", "ups": "2.72", "wpb": "323.8", "bsz": "16", "num_updates": "14600", "lr": "4.92946e-05", "gnorm": "2.396", "train_wall": "36", "wall": "7558"}
2021-10-25 00:49:59 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-25 00:49:59 | INFO | train_inner | {"epoch": 12, "update": 12.0, "loss": "2.21", "nll_loss": "0.289", "ppl": "1.22", "wps": "901.2", "ups": "2.79", "wpb": "323.1", "bsz": "15.9", "num_updates": "14700", "lr": "4.92896e-05", "gnorm": "2.374", "train_wall": "36", "wall": "7593"}
2021-10-25 00:49:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-10-25 00:53:28 | INFO | valid | {"epoch": 12, "valid_loss": "3.057", "valid_nll_loss": "1.088", "valid_ppl": "2.13", "valid_bleu": "60.6", "valid_wps": "214.3", "valid_wpb": "72.4", "valid_bsz": "4", "valid_num_updates": "14700", "valid_best_bleu": "67.26"}
2021-10-25 00:53:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-10-25 00:53:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../models/plbart/unique/large/checkpoint_last.pt (epoch 12 @ 14700 updates, score 60.6) (writing took 15.40533137100283 seconds)
2021-10-25 00:53:43 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-10-25 00:53:44 | INFO | train | {"epoch": 12, "train_loss": "2.199", "train_nll_loss": "0.272", "train_ppl": "1.21", "train_wps": "571.4", "train_ups": "1.87", "train_wpb": "305.1", "train_bsz": "16", "train_num_updates": "14700", "train_lr": "4.92896e-05", "train_gnorm": "2.43", "train_train_wall": "425", "train_wall": "7818"}
2021-10-25 00:53:44 | INFO | fairseq_cli.train | begin training epoch 12
2021-10-25 00:54:23 | INFO | train_inner | {"epoch": 13, "update": 12.082, "loss": "2.175", "nll_loss": "0.245", "ppl": "1.18", "wps": "118.5", "ups": "0.38", "wpb": "312.1", "bsz": "16", "num_updates": "14800", "lr": "4.92846e-05", "gnorm": "1.948", "train_wall": "37", "wall": "7857"}
2021-10-25 00:55:01 | INFO | train_inner | {"epoch": 13, "update": 12.163, "loss": "2.188", "nll_loss": "0.261", "ppl": "1.2", "wps": "724.8", "ups": "2.62", "wpb": "276.8", "bsz": "16", "num_updates": "14900", "lr": "4.92796e-05", "gnorm": "2.414", "train_wall": "38", "wall": "7895"}
2021-10-25 00:55:38 | INFO | train_inner | {"epoch": 13, "update": 12.245, "loss": "2.181", "nll_loss": "0.253", "ppl": "1.19", "wps": "737.3", "ups": "2.68", "wpb": "274.9", "bsz": "16", "num_updates": "15000", "lr": "4.92746e-05", "gnorm": "2.298", "train_wall": "37", "wall": "7932"}
2021-10-25 00:56:17 | INFO | train_inner | {"epoch": 13, "update": 12.327, "loss": "2.187", "nll_loss": "0.262", "ppl": "1.2", "wps": "825.9", "ups": "2.58", "wpb": "320.2", "bsz": "16", "num_updates": "15100", "lr": "4.92696e-05", "gnorm": "2.58", "train_wall": "39", "wall": "7971"}
2021-10-25 00:56:49 | INFO | train_inner | {"epoch": 13, "update": 12.408, "loss": "2.182", "nll_loss": "0.255", "ppl": "1.19", "wps": "883.3", "ups": "3.07", "wpb": "287.4", "bsz": "16", "num_updates": "15200", "lr": "4.92646e-05", "gnorm": "2.217", "train_wall": "32", "wall": "8004"}
2021-10-25 00:57:26 | INFO | train_inner | {"epoch": 13, "update": 12.49, "loss": "2.189", "nll_loss": "0.264", "ppl": "1.2", "wps": "795.9", "ups": "2.71", "wpb": "293.2", "bsz": "16", "num_updates": "15300", "lr": "4.92596e-05", "gnorm": "2.153", "train_wall": "37", "wall": "8041"}
2021-10-25 00:58:00 | INFO | train_inner | {"epoch": 13, "update": 12.571, "loss": "2.199", "nll_loss": "0.275", "ppl": "1.21", "wps": "812.1", "ups": "3", "wpb": "270.3", "bsz": "16", "num_updates": "15400", "lr": "4.92546e-05", "gnorm": "2.419", "train_wall": "33", "wall": "8074"}
2021-10-25 00:58:37 | INFO | train_inner | {"epoch": 13, "update": 12.653, "loss": "2.195", "nll_loss": "0.274", "ppl": "1.21", "wps": "916.3", "ups": "2.69", "wpb": "341", "bsz": "16", "num_updates": "15500", "lr": "4.92496e-05", "gnorm": "2.344", "train_wall": "37", "wall": "8111"}
2021-10-25 00:59:11 | INFO | train_inner | {"epoch": 13, "update": 12.735, "loss": "2.206", "nll_loss": "0.281", "ppl": "1.22", "wps": "838.3", "ups": "2.94", "wpb": "285.3", "bsz": "16", "num_updates": "15600", "lr": "4.92446e-05", "gnorm": "2.607", "train_wall": "34", "wall": "8145"}
2021-10-25 00:59:47 | INFO | train_inner | {"epoch": 13, "update": 12.816, "loss": "2.215", "nll_loss": "0.293", "ppl": "1.23", "wps": "877.9", "ups": "2.8", "wpb": "313.1", "bsz": "16", "num_updates": "15700", "lr": "4.92396e-05", "gnorm": "2.417", "train_wall": "35", "wall": "8181"}
2021-10-25 01:00:22 | INFO | train_inner | {"epoch": 13, "update": 12.898, "loss": "2.197", "nll_loss": "0.278", "ppl": "1.21", "wps": "969.7", "ups": "2.79", "wpb": "347.8", "bsz": "16", "num_updates": "15800", "lr": "4.92346e-05", "gnorm": "2.273", "train_wall": "36", "wall": "8217"}
2021-10-25 01:01:00 | INFO | train_inner | {"epoch": 13, "update": 12.98, "loss": "2.212", "nll_loss": "0.293", "ppl": "1.22", "wps": "928.2", "ups": "2.68", "wpb": "346.6", "bsz": "16", "num_updates": "15900", "lr": "4.92296e-05", "gnorm": "2.742", "train_wall": "37", "wall": "8254"}
2021-10-25 01:01:08 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the number of workers (--num-workers) may help.
2021-10-25 01:01:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
../../data/unique/split/large/tgt-test.txt
../models/plbart/unique/large/output
